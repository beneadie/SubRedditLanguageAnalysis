{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58fd72cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\miniconda3\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c3c916",
   "metadata": {},
   "source": [
    "## Pulling text data from subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f7b88ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most came from this link\n",
    "#https://www.reddit.com/r/TheoryOfReddit/comments/1f7hqc/the_200_most_active_subreddits_categorized_by/\n",
    "\n",
    "DiscussionSubs = [\"CrazyIdeas\", \"AskReddit\", \"fatpeoplestories\", \"DoesAnybodyElse\", \"IAmA\", \"bestof\", \"TalesFromRetail\"]\n",
    "GamingSubs = [\"gaming\", \"leagueoflegends\", \"pokemon\", \"Minecraft\", \"starcraft\", \"Games\", \"DotA2\", \"skyrim\", \"tf2\", \"magicTCG\", \"wow\", \"KerbalSpaceProgram\", \"mindcrack\"]\n",
    "TVSubs = [\"arresteddevelopment\", \"gameofthrones\", \"doctorwho\", \"mylittlepony\", \"community\", \"breakingbad\", \"adventuretime\"]\n",
    "MoviesSubs = [\"movies\", \"harrypotter\", \"StarWars\", \"anime\", \"batman\", \"moviecritic\", \"MovieSuggestions\", \"Hungergames\", \"MarvelStudiosSpoilers\"]\n",
    "NewsSubs = [\"politics\", \"worldnews\", \"news\", \"conspiracy\", \"Libertarian\", \"offbeat\", \"TrueReddit\", \"Conservative\"]\n",
    "HumourSubs = [\"funny\", \"fffffffuuuuuuuuuuuu\", \"4chan\", \"lmGoingToHellForThis\", \"firstworldanarchists\", \"circlejerk\", \"okbuddyretard\", \"facepalm\", \"Jokes\"]\n",
    "SportsSubs = [\"nba\", \"soccer\", \"hockey\", \"nfl\", \"formula1\", \"baseball\", \"MMA\", \"rugbyunion\", \"PremierLeague\"]\n",
    "LearningSubs = [\"todayilearned\", \"science\", \"askscience\", \"space\", \"AskHistorians\", \"YouShouldKnow\", \"explainlikeimfive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb4749b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CrazyIdeas', 'AskReddit', 'fatpeoplestories', 'DoesAnybodyElse',\n",
       "       'IAmA', 'bestof', 'TalesFromRetail', 'gaming', 'leagueoflegends',\n",
       "       'pokemon', 'Minecraft', 'starcraft', 'Games', 'DotA2', 'skyrim',\n",
       "       'tf2', 'magicTCG', 'wow', 'KerbalSpaceProgram', 'mindcrack',\n",
       "       'arresteddevelopment', 'gameofthrones', 'doctorwho',\n",
       "       'mylittlepony', 'community', 'breakingbad', 'adventuretime',\n",
       "       'movies', 'harrypotter', 'StarWars', 'anime', 'batman',\n",
       "       'moviecritic', 'MovieSuggestions', 'Hungergames',\n",
       "       'MarvelStudiosSpoilers', 'politics', 'worldnews', 'news',\n",
       "       'conspiracy', 'Libertarian', 'offbeat', 'TrueReddit',\n",
       "       'Conservative', 'funny', 'fffffffuuuuuuuuuuuu', '4chan',\n",
       "       'ImGoingToHellForThis', 'firstworldanarchists', 'circlejerk',\n",
       "       'okbuddyretard', 'facepalm', 'Jokes', 'nba', 'soccer', 'hockey',\n",
       "       'nfl', 'formula1', 'baseball', 'MMA', 'rugbyunion',\n",
       "       'PremierLeague', 'todayilearned', 'science', 'askscience', 'space',\n",
       "       'AskHistorians', 'YouShouldKnow', 'explainlikeimfive'],\n",
       "      dtype='<U21')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrs = (DiscussionSubs, GamingSubs, TVSubs, MoviesSubs, NewsSubs, HumourSubs, SportsSubs, LearningSubs)\n",
    "\n",
    "BIGLIST = np.concatenate(arrs)\n",
    "BIGLIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2c09f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = \"wNnSYWwhkIvYFOgtGkt7eg\" #your client id\n",
    "cs = \"Jm3VihJQnFsivAevI3iAR0iT8xzhWQ\" #your client secret\n",
    "ua = \"anelka39\" #your user agent name\n",
    "#sub = \"okbuddyretard\" #the name of the subreddit (not including the 'r/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b5dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "56e57a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed downloading text from r/CrazyIdeas\n",
      "completed downloading text from r/AskReddit\n",
      "completed downloading text from r/fatpeoplestories\n",
      "completed downloading text from r/DoesAnybodyElse\n",
      "completed downloading text from r/IAmA\n",
      "completed downloading text from r/bestof\n",
      "completed downloading text from r/TalesFromRetail\n",
      "completed downloading text from r/gaming\n",
      "completed downloading text from r/leagueoflegends\n",
      "completed downloading text from r/pokemon\n",
      "completed downloading text from r/Minecraft\n",
      "completed downloading text from r/starcraft\n",
      "completed downloading text from r/Games\n",
      "completed downloading text from r/DotA2\n",
      "completed downloading text from r/skyrim\n",
      "completed downloading text from r/tf2\n",
      "completed downloading text from r/magicTCG\n",
      "completed downloading text from r/wow\n",
      "completed downloading text from r/KerbalSpaceProgram\n",
      "completed downloading text from r/mindcrack\n",
      "completed downloading text from r/arresteddevelopment\n",
      "completed downloading text from r/gameofthrones\n",
      "completed downloading text from r/doctorwho\n",
      "completed downloading text from r/mylittlepony\n",
      "completed downloading text from r/community\n",
      "completed downloading text from r/breakingbad\n",
      "completed downloading text from r/adventuretime\n",
      "completed downloading text from r/movies\n",
      "completed downloading text from r/harrypotter\n",
      "completed downloading text from r/StarWars\n",
      "completed downloading text from r/anime\n",
      "completed downloading text from r/batman\n",
      "completed downloading text from r/moviecritic\n",
      "completed downloading text from r/MovieSuggestions\n",
      "completed downloading text from r/Hungergames\n",
      "completed downloading text from r/MarvelStudiosSpoilers\n",
      "completed downloading text from r/politics\n",
      "completed downloading text from r/worldnews\n",
      "completed downloading text from r/news\n",
      "completed downloading text from r/conspiracy\n",
      "completed downloading text from r/Libertarian\n",
      "completed downloading text from r/offbeat\n",
      "completed downloading text from r/TrueReddit\n",
      "completed downloading text from r/Conservative\n",
      "completed downloading text from r/funny\n",
      "completed downloading text from r/fffffffuuuuuuuuuuuu\n",
      "completed downloading text from r/4chan\n"
     ]
    },
    {
     "ename": "Forbidden",
     "evalue": "received 403 HTTP response",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [60], line 14\u001b[0m\n\u001b[0;32m      3\u001b[0m reddit \u001b[38;5;241m=\u001b[39m praw\u001b[38;5;241m.\u001b[39mReddit(\n\u001b[0;32m      4\u001b[0m     client_id\u001b[38;5;241m=\u001b[39mci,\n\u001b[0;32m      5\u001b[0m     client_secret\u001b[38;5;241m=\u001b[39mcs,\n\u001b[0;32m      6\u001b[0m     user_agent\u001b[38;5;241m=\u001b[39mua\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(sub\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m#on the following line you can change top to any of the previously mentioned ways of sorting \u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#and the limit to however many posts you would like to extract (here we extract just 10).\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m post \u001b[38;5;129;01min\u001b[39;00m reddit\u001b[38;5;241m.\u001b[39msubreddit(sub)\u001b[38;5;241m.\u001b[39mtop(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m): \n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m         \u001b[38;5;66;03m#this line collects the post titles\u001b[39;00m\n\u001b[0;32m     17\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(post\u001b[38;5;241m.\u001b[39mtitle\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;66;03m#this line collects the post content\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\praw\\models\\listing\\generator.py:63\u001b[0m, in \u001b[0;36mListingGenerator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing):\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\praw\\models\\listing\\generator.py:89\u001b[0m, in \u001b[0;36mListingGenerator._next_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exhausted:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reddit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_sublist(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     arg_string \u001b[38;5;241m=\u001b[39m _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[0;32m     37\u001b[0m     warn(\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     42\u001b[0m     )\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(_old_args, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\praw\\reddit.py:712\u001b[0m, in \u001b[0;36mReddit.get\u001b[1;34m(self, path, params)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;129m@_deprecate_args\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    704\u001b[0m     params: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    705\u001b[0m ):\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;124;03m\"\"\"Return parsed objects returned from a GET request to ``path``.\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \n\u001b[0;32m    708\u001b[0m \u001b[38;5;124;03m    :param path: The path to fetch.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03m    :param params: The query parameters to add to the request (default: ``None``).\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \n\u001b[0;32m    711\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 712\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_objectify_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\praw\\reddit.py:517\u001b[0m, in \u001b[0;36mReddit._objectify_request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_objectify_request\u001b[39m(\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    499\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    500\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;124;03m\"\"\"Run a request through the ``Objector``.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \n\u001b[0;32m    503\u001b[0m \u001b[38;5;124;03m    :param data: Dictionary, bytes, or file-like object to send in the body of the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m \n\u001b[0;32m    515\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objector\u001b[38;5;241m.\u001b[39mobjectify(\n\u001b[1;32m--> 517\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     arg_string \u001b[38;5;241m=\u001b[39m _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[0;32m     37\u001b[0m     warn(\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     42\u001b[0m     )\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(_old_args, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\praw\\reddit.py:941\u001b[0m, in \u001b[0;36mReddit.request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt most one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 941\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequest \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\prawcore\\sessions.py:330\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m     json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    329\u001b[0m url \u001b[38;5;241m=\u001b[39m urljoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requestor\u001b[38;5;241m.\u001b[39moauth_url, path)\n\u001b[1;32m--> 330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\prawcore\\sessions.py:266\u001b[0m, in \u001b[0;36mSession._request_with_retries\u001b[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_retry(\n\u001b[0;32m    254\u001b[0m         data,\n\u001b[0;32m    255\u001b[0m         files,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m         url,\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTATUS_EXCEPTIONS:\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTATUS_EXCEPTIONS[response\u001b[38;5;241m.\u001b[39mstatus_code](response)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m codes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_content\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mForbidden\u001b[0m: received 403 HTTP response"
     ]
    }
   ],
   "source": [
    "for i in BIGLIST:\n",
    "    sub = i\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=ci,\n",
    "        client_secret=cs,\n",
    "        user_agent=ua\n",
    "    )\n",
    "\n",
    "    with open(sub+\".txt\", \"w\", encoding='utf-8') as f:\n",
    "\n",
    "\n",
    "        #on the following line you can change top to any of the previously mentioned ways of sorting \n",
    "        #and the limit to however many posts you would like to extract (here we extract just 10).\n",
    "        for post in reddit.subreddit(sub).top(limit=10): \n",
    "\n",
    "            #this line collects the post titles\n",
    "            f.write(post.title+\"\\n\")\n",
    "\n",
    "            #this line collects the post content\n",
    "            f.write(post.selftext+\"\\n\")\n",
    "\n",
    "            #this section collects the comments\n",
    "            for comment in post.comments:\n",
    "                if isinstance(comment, MoreComments):\n",
    "                    continue\n",
    "                f.write(comment.body+\"\\n\")\n",
    "        print(f\"completed downloading text from r/{sub}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f998fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed downloading text from r/lmGoingToHellForThis\n"
     ]
    }
   ],
   "source": [
    "sub = \"lmGoingToHellForThis\"\n",
    "reddit = praw.Reddit(\n",
    "    client_id=ci,\n",
    "    client_secret=cs,\n",
    "    user_agent=ua\n",
    ")\n",
    "\n",
    "with open(sub+\".txt\", \"w\", encoding='utf-8') as f:\n",
    "\n",
    "\n",
    "    #on the following line you can change top to any of the previously mentioned ways of sorting \n",
    "    #and the limit to however many posts you would like to extract (here we extract just 10).\n",
    "    for post in reddit.subreddit(sub).top(limit=10): \n",
    "\n",
    "        #this line collects the post titles\n",
    "        f.write(post.title+\"\\n\")\n",
    "\n",
    "        #this line collects the post content\n",
    "        f.write(post.selftext+\"\\n\")\n",
    "\n",
    "        #this section collects the comments\n",
    "        for comment in post.comments:\n",
    "            if isinstance(comment, MoreComments):\n",
    "                continue\n",
    "            f.write(comment.body+\"\\n\")\n",
    "    print(f\"completed downloading text from r/{sub}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "265073cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "afterbreakList= ['firstworldanarchists', 'circlejerk','okbuddyretard', 'facepalm', 'Jokes', 'nba', 'soccer', 'hockey', 'nfl', 'formula1', 'baseball', 'MMA', 'rugbyunion','PremierLeague', 'todayilearned', 'science', 'askscience', 'space','AskHistorians', 'YouShouldKnow', 'explainlikeimfive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bfe87da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed downloading text from r/firstworldanarchists\n",
      "completed downloading text from r/circlejerk\n",
      "completed downloading text from r/okbuddyretard\n",
      "completed downloading text from r/facepalm\n",
      "completed downloading text from r/Jokes\n",
      "completed downloading text from r/nba\n",
      "completed downloading text from r/soccer\n",
      "completed downloading text from r/hockey\n",
      "completed downloading text from r/nfl\n",
      "completed downloading text from r/formula1\n",
      "completed downloading text from r/baseball\n",
      "completed downloading text from r/MMA\n",
      "completed downloading text from r/rugbyunion\n",
      "completed downloading text from r/PremierLeague\n",
      "completed downloading text from r/todayilearned\n",
      "completed downloading text from r/science\n",
      "completed downloading text from r/askscience\n",
      "completed downloading text from r/space\n",
      "completed downloading text from r/AskHistorians\n",
      "completed downloading text from r/YouShouldKnow\n",
      "completed downloading text from r/explainlikeimfive\n"
     ]
    }
   ],
   "source": [
    "for i in afterbreakList:\n",
    "    sub = i\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=ci,\n",
    "        client_secret=cs,\n",
    "        user_agent=ua\n",
    "    )\n",
    "\n",
    "    with open(sub+\".txt\", \"w\", encoding='utf-8') as f:\n",
    "\n",
    "\n",
    "        #on the following line you can change top to any of the previously mentioned ways of sorting \n",
    "        #and the limit to however many posts you would like to extract (here we extract just 10).\n",
    "        for post in reddit.subreddit(sub).top(limit=10): \n",
    "\n",
    "            #this line collects the post titles\n",
    "            f.write(post.title+\"\\n\")\n",
    "\n",
    "            #this line collects the post content\n",
    "            f.write(post.selftext+\"\\n\")\n",
    "\n",
    "            #this section collects the comments\n",
    "            for comment in post.comments:\n",
    "                if isinstance(comment, MoreComments):\n",
    "                    continue\n",
    "                f.write(comment.body+\"\\n\")\n",
    "        print(f\"completed downloading text from r/{sub}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdbec8a",
   "metadata": {},
   "source": [
    "## Sourcing Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1854c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt(file):\n",
    "    with open(file, encoding='utf8') as f:\n",
    "        return ' '.join(line.strip() for line in f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a34199fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "import statistics\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c173f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a1fd7",
   "metadata": {},
   "source": [
    "## Initial Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd719310",
   "metadata": {},
   "source": [
    "### Sports Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23fe3ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   nba\n",
      "Original: 0.14740657633554846\n",
      "Processed 0.25050970075633017\n",
      "Processing:   soccer\n",
      "Original: 0.1853431930223739\n",
      "Processed 0.3033372427764154\n",
      "Processing:   hockey\n",
      "Original: 0.15913278062903013\n",
      "Processed 0.2702212823804849\n",
      "Processing:   nfl\n",
      "Original: 0.19370311782415675\n",
      "Processed 0.31456245072643313\n",
      "Processing:   formula1\n",
      "Original: 0.1392944590550467\n",
      "Processed 0.23590757276259286\n",
      "Processing:   baseball\n",
      "Original: 0.1503168994335072\n",
      "Processed 0.249815918066805\n",
      "Processing:   MMA\n",
      "Original: 0.15526559374160223\n",
      "Processed 0.2654003076717276\n",
      "Processing:   rugbyunion\n",
      "Original: 0.22897550298846706\n",
      "Processed 0.39006282124500286\n",
      "Processing:   PremierLeague\n",
      "Original: 0.15418424235273295\n",
      "Processed 0.2707836907598623\n",
      "Average Orignal Score is:    0.16818026282027393\n",
      "Average Processed Score is:    0.28340010968285045\n",
      "Min Orignal Score is:    0.1392944590550467\n",
      "Min Processed Score is:    0.23590757276259286\n",
      "Max Orignal Score is:    0.22897550298846706\n",
      "Max Processed Score is:    0.39006282124500286\n",
      "CPU times: total: 35.5 s\n",
      "Wall time: 36.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "recordOriginal = []\n",
    "recordPros =[]\n",
    "\n",
    "SportsFDarray = []\n",
    "\n",
    "for i in SportsSubs:\n",
    "    print(\"Processing:  \", i)\n",
    "    ulysses_txt = load_txt(f'{i}.txt')\n",
    "    ulysses_tokens = nltk.word_tokenize(ulysses_txt)\n",
    "\n",
    "\n",
    "\n",
    "    ulysses_lowercase = [t.lower() for t in ulysses_tokens]\n",
    "    ulysses_no_punct = [t for t in ulysses_lowercase if t.isalnum()]\n",
    "    ulysses_no_stop = [t for t in ulysses_no_punct if t not in stopwords.words('english')]\n",
    "\n",
    "    orig = lexical_diversity(ulysses_tokens)\n",
    "    pros = lexical_diversity(ulysses_no_stop)\n",
    "    print(\"Original:\", orig)\n",
    "    print(\"Processed\", pros)\n",
    "    \n",
    "    recordOriginal.append(orig)\n",
    "    recordPros.append(pros)\n",
    "\n",
    "    fdist1 = nltk.FreqDist(ulysses_no_stop)\n",
    "    top_75 = fdist1.most_common(75)\n",
    "    \n",
    "    top_words =[]\n",
    "    for i in top_75:\n",
    "        top_words.append(i[0])\n",
    "    \n",
    "    SportsFDarray.append(top_words)\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"Average Orignal Score is:   \", statistics.mean(recordOriginal))\n",
    "print(\"Average Processed Score is:   \", statistics.mean(recordPros))\n",
    "\n",
    "print(\"Min Orignal Score is:   \", min(recordOriginal))\n",
    "print(\"Min Processed Score is:   \", min(recordPros))\n",
    "\n",
    "print(\"Max Orignal Score is:   \", max(recordOriginal))\n",
    "print(\"Max Processed Score is:   \", max(recordPros))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84d87f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['league',\n",
       " 'like',\n",
       " 'football',\n",
       " 'game',\n",
       " 'fans',\n",
       " 'get',\n",
       " 'chelsea',\n",
       " 'fan',\n",
       " 'one',\n",
       " 'think',\n",
       " 'got',\n",
       " 'even',\n",
       " 'arsenal',\n",
       " 'big',\n",
       " 'would',\n",
       " 'clubs',\n",
       " 'foul',\n",
       " 'refs',\n",
       " 'man',\n",
       " 'every',\n",
       " 'see',\n",
       " 'fuck',\n",
       " 'team',\n",
       " 'var',\n",
       " 'shit',\n",
       " 'could',\n",
       " 'going',\n",
       " 'go',\n",
       " 'fucking',\n",
       " 'back',\n",
       " 'time',\n",
       " 'love',\n",
       " 'play',\n",
       " 'know',\n",
       " 'city',\n",
       " 'good',\n",
       " 'goal',\n",
       " 'liverpool',\n",
       " 'player',\n",
       " 'ball',\n",
       " 'us',\n",
       " '6',\n",
       " 'teams',\n",
       " 'ref',\n",
       " 'da',\n",
       " 'penalty',\n",
       " 'watch',\n",
       " 'premier',\n",
       " 'super',\n",
       " 'thing',\n",
       " 'club',\n",
       " 'well',\n",
       " 'season',\n",
       " 'best',\n",
       " 'red',\n",
       " 'also',\n",
       " 'bias',\n",
       " 'pen',\n",
       " 'need',\n",
       " 'top',\n",
       " 'never',\n",
       " 'andersen',\n",
       " 'players',\n",
       " 'win',\n",
       " 'richarlison',\n",
       " 'united',\n",
       " 'make',\n",
       " 'support',\n",
       " 'tune',\n",
       " 'chant',\n",
       " 'money',\n",
       " 'really',\n",
       " 'nunez',\n",
       " 'let',\n",
       " 'take']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SportsFDarray[-1] # top word from every single array - not what exactly i wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "032c6660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kobe', 'https', 'like', 'one', 'man', 'nba', 'people', 'time', 'never', 'game']\n",
      "['messi', 'like', 'fucking', 'goal', 'one', 'football', 'world', 'game', 'team', 'fuck']\n",
      "['like', 'fuck', 'putin', 'kobe', 'one', 'hockey', 'game', 'good', 'https', 'fucking']\n",
      "['brady', 'game', 'https', 'tom', 'like', 'fucking', 'time', 'one', 'madden', 'play']\n",
      "['race', 'win', 'max', 'like', 'get', 'lewis', 'one', 'see', 'f1', 'leclerc']\n",
      "['baseball', 'day', 'one', 'https', 'game', 'love', 'first', 'dad', 'time', 'fuck']\n",
      "['fight', 'jones', 'like', 'jon', 'ufc', 'conor', 'get', 'one', 'shit', 'time']\n",
      "['like', 'try', 'rugby', 'would', 'get', 'one', 'forward', 'whale', 'shit', 'man']\n",
      "['league', 'like', 'football', 'game', 'fans', 'get', 'chelsea', 'fan', 'one', 'think']\n"
     ]
    }
   ],
   "source": [
    "for i in SportsFDarray:\n",
    "    print(i[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a10d38ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kobe',\n",
       " 'https',\n",
       " 'like',\n",
       " 'one',\n",
       " 'man',\n",
       " 'nba',\n",
       " 'people',\n",
       " 'time',\n",
       " 'never',\n",
       " 'game',\n",
       " 'life',\n",
       " 'daughter',\n",
       " 'get',\n",
       " 'fuck',\n",
       " 'bryant',\n",
       " 'even',\n",
       " 'rip',\n",
       " 'love',\n",
       " 'know',\n",
       " 'shit',\n",
       " 'day',\n",
       " 'basketball',\n",
       " 'much',\n",
       " 'fucking',\n",
       " 'right',\n",
       " 'today',\n",
       " 'would',\n",
       " 'going',\n",
       " 'got',\n",
       " 'years',\n",
       " 'family',\n",
       " 'really',\n",
       " 'lebron',\n",
       " 'chuck',\n",
       " 'still',\n",
       " 'see',\n",
       " 'post',\n",
       " 'helicopter',\n",
       " 'crash',\n",
       " 'us',\n",
       " 'way',\n",
       " 'back',\n",
       " 'ca',\n",
       " 'first',\n",
       " 'feel',\n",
       " 'also',\n",
       " 'ever',\n",
       " 'player',\n",
       " 'real',\n",
       " 'last',\n",
       " 'think',\n",
       " 'best',\n",
       " 'everyone',\n",
       " 'always',\n",
       " 'believe',\n",
       " 'history',\n",
       " 'death',\n",
       " 'every',\n",
       " 'well',\n",
       " 'lakers',\n",
       " 'fan',\n",
       " 'top',\n",
       " 'died',\n",
       " 'board',\n",
       " 'said',\n",
       " 'say',\n",
       " 'deleted',\n",
       " 'make',\n",
       " 'go',\n",
       " 'good',\n",
       " 'edit',\n",
       " 'mamba',\n",
       " 'gianna',\n",
       " 'things',\n",
       " 'let',\n",
       " 'messi',\n",
       " 'like',\n",
       " 'fucking',\n",
       " 'goal',\n",
       " 'one',\n",
       " 'football',\n",
       " 'world',\n",
       " 'game',\n",
       " 'team',\n",
       " 'fuck',\n",
       " 'final',\n",
       " 'ever',\n",
       " 'time',\n",
       " 'best',\n",
       " 'argentina',\n",
       " 'would',\n",
       " 'post',\n",
       " 'cup',\n",
       " 'league',\n",
       " 'https',\n",
       " 'even',\n",
       " 'get',\n",
       " 'mother',\n",
       " 'soccer',\n",
       " 'leicester',\n",
       " 'think',\n",
       " 'good',\n",
       " 'deleted',\n",
       " 'see',\n",
       " 'city',\n",
       " 'going',\n",
       " 'shit',\n",
       " 'ronaldo',\n",
       " 'mom',\n",
       " 'lmao',\n",
       " 'aguero',\n",
       " 'player',\n",
       " 'lol',\n",
       " 'years',\n",
       " 'oh',\n",
       " 'say',\n",
       " 'go',\n",
       " 'right',\n",
       " 'people',\n",
       " 'know',\n",
       " 'never',\n",
       " 'seen',\n",
       " 'got',\n",
       " 'already',\n",
       " 'history',\n",
       " 'well',\n",
       " 'better',\n",
       " 'donkey',\n",
       " 'holy',\n",
       " 'mbappe',\n",
       " 'could',\n",
       " 'really',\n",
       " 'first',\n",
       " 'last',\n",
       " 'man',\n",
       " 'bale',\n",
       " '2',\n",
       " 'much',\n",
       " 'please',\n",
       " 'greatest',\n",
       " 'still',\n",
       " 'real',\n",
       " 'sub',\n",
       " 'win',\n",
       " 'winning',\n",
       " 'love',\n",
       " 'back',\n",
       " 'god',\n",
       " 'cook',\n",
       " 'thread',\n",
       " 'like',\n",
       " 'fuck',\n",
       " 'putin',\n",
       " 'kobe',\n",
       " 'one',\n",
       " 'hockey',\n",
       " 'game',\n",
       " 'good',\n",
       " 'https',\n",
       " 'fucking',\n",
       " 'first',\n",
       " 'get',\n",
       " 'time',\n",
       " 'even',\n",
       " 'see',\n",
       " 'hit',\n",
       " 'cup',\n",
       " 'could',\n",
       " 'go',\n",
       " 'would',\n",
       " 'got',\n",
       " 'team',\n",
       " 'shit',\n",
       " 'never',\n",
       " 'guy',\n",
       " 'love',\n",
       " 'years',\n",
       " 'still',\n",
       " 'rip',\n",
       " 'play',\n",
       " 'right',\n",
       " 'people',\n",
       " 'going',\n",
       " 'blues',\n",
       " 'way',\n",
       " 'fan',\n",
       " 'really',\n",
       " 'man',\n",
       " 'puck',\n",
       " 'well',\n",
       " 'great',\n",
       " 'also',\n",
       " 'much',\n",
       " 'war',\n",
       " 'season',\n",
       " 'hope',\n",
       " 'year',\n",
       " 'basketball',\n",
       " 'better',\n",
       " 'family',\n",
       " 'ovechkin',\n",
       " 'marchand',\n",
       " 'scheifele',\n",
       " 'win',\n",
       " 'ovi',\n",
       " 'history',\n",
       " 'nhl',\n",
       " 'russia',\n",
       " 'last',\n",
       " 'post',\n",
       " 'say',\n",
       " 'know',\n",
       " 'think',\n",
       " 'something',\n",
       " 'feel',\n",
       " 'happy',\n",
       " 'legend',\n",
       " 'someone',\n",
       " 'said',\n",
       " 'looks',\n",
       " 'series',\n",
       " 'rest',\n",
       " 'players',\n",
       " 'day',\n",
       " 'ever',\n",
       " 'brady',\n",
       " 'game',\n",
       " 'https',\n",
       " 'tom',\n",
       " 'like',\n",
       " 'fucking',\n",
       " 'time',\n",
       " 'one',\n",
       " 'madden',\n",
       " 'play',\n",
       " 'super',\n",
       " 'fuck',\n",
       " 'nfl',\n",
       " 'football',\n",
       " 'romo',\n",
       " 'bowl',\n",
       " 'get',\n",
       " 'goat',\n",
       " 'ever',\n",
       " 'man',\n",
       " 'people',\n",
       " 'shit',\n",
       " 'love',\n",
       " 'patriots',\n",
       " 'said',\n",
       " 'know',\n",
       " 'john',\n",
       " 'would',\n",
       " 'years',\n",
       " 'good',\n",
       " 'best',\n",
       " 'first',\n",
       " 'rodgers',\n",
       " 'back',\n",
       " 'miami',\n",
       " 'rip',\n",
       " 'win',\n",
       " 'greatest',\n",
       " 'never',\n",
       " 'gronk',\n",
       " 'think',\n",
       " 'superbowl',\n",
       " 'holy',\n",
       " 'way',\n",
       " 'fan',\n",
       " 'could',\n",
       " 'still',\n",
       " 'bengals',\n",
       " 'harambe',\n",
       " 'deleted',\n",
       " 'got',\n",
       " 'really',\n",
       " 'guy',\n",
       " 'see',\n",
       " 'joe',\n",
       " 'go',\n",
       " 'career',\n",
       " '3',\n",
       " 'let',\n",
       " 'right',\n",
       " 'coach',\n",
       " 'made',\n",
       " 'year',\n",
       " 'great',\n",
       " 'history',\n",
       " 'call',\n",
       " 'season',\n",
       " '2',\n",
       " 'make',\n",
       " 'last',\n",
       " 'dolphins',\n",
       " 'team',\n",
       " 'rest',\n",
       " 'td',\n",
       " 'going',\n",
       " 'race',\n",
       " 'win',\n",
       " 'max',\n",
       " 'like',\n",
       " 'get',\n",
       " 'lewis',\n",
       " 'one',\n",
       " 'see',\n",
       " 'f1',\n",
       " 'leclerc',\n",
       " 'would',\n",
       " 'pierre',\n",
       " 'people',\n",
       " 'charles',\n",
       " 'hamilton',\n",
       " 'time',\n",
       " 'got',\n",
       " 'last',\n",
       " 'ferrari',\n",
       " 'gasly',\n",
       " 'pole',\n",
       " 'think',\n",
       " 'well',\n",
       " 'fuck',\n",
       " 'year',\n",
       " 'know',\n",
       " 'even',\n",
       " 'fucking',\n",
       " 'happy',\n",
       " 'first',\n",
       " 'season',\n",
       " 'go',\n",
       " 'going',\n",
       " 'never',\n",
       " 'post',\n",
       " 'good',\n",
       " 'top',\n",
       " 'really',\n",
       " 'car',\n",
       " 'someone',\n",
       " 'carlos',\n",
       " 'also',\n",
       " 'man',\n",
       " 'kid',\n",
       " 'reddit',\n",
       " 'driver',\n",
       " 'love',\n",
       " 'great',\n",
       " 'bottas',\n",
       " 'make',\n",
       " 'much',\n",
       " 'verstappen',\n",
       " 'guy',\n",
       " 'wins',\n",
       " 'back',\n",
       " 'mclaren',\n",
       " 'today',\n",
       " 'new',\n",
       " 'every',\n",
       " 'shit',\n",
       " 'day',\n",
       " 'let',\n",
       " 'want',\n",
       " 'could',\n",
       " 'way',\n",
       " 'cars',\n",
       " 'always',\n",
       " 'say',\n",
       " 'bullied',\n",
       " 'na',\n",
       " 'take',\n",
       " 'congrats',\n",
       " 'hope',\n",
       " 'red',\n",
       " 'bullies',\n",
       " 'baseball',\n",
       " 'day',\n",
       " 'one',\n",
       " 'https',\n",
       " 'game',\n",
       " 'love',\n",
       " 'first',\n",
       " 'dad',\n",
       " 'time',\n",
       " 'fuck',\n",
       " 'http',\n",
       " 'astros',\n",
       " 'thank',\n",
       " 'trout',\n",
       " 'see',\n",
       " 'cubs',\n",
       " 'best',\n",
       " 'know',\n",
       " 'get',\n",
       " 'like',\n",
       " 'would',\n",
       " 'mike',\n",
       " 'series',\n",
       " 'world',\n",
       " 'play',\n",
       " 'years',\n",
       " 'ever',\n",
       " 'got',\n",
       " 'good',\n",
       " 'even',\n",
       " 'every',\n",
       " 'last',\n",
       " 'man',\n",
       " 'fans',\n",
       " 'dale',\n",
       " 'murphy',\n",
       " 'still',\n",
       " 'lockout',\n",
       " 'team',\n",
       " 'fucking',\n",
       " 'kelly',\n",
       " 'much',\n",
       " 'could',\n",
       " 'fan',\n",
       " 'think',\n",
       " 'year',\n",
       " 'going',\n",
       " 'go',\n",
       " 'joe',\n",
       " 'player',\n",
       " 'favorite',\n",
       " '1',\n",
       " 'never',\n",
       " 'watch',\n",
       " 'really',\n",
       " 'made',\n",
       " 'back',\n",
       " 'us',\n",
       " 'manfred',\n",
       " 'hall',\n",
       " 'many',\n",
       " 'hope',\n",
       " 'right',\n",
       " 'since',\n",
       " 'season',\n",
       " 'yet',\n",
       " 'great',\n",
       " 'players',\n",
       " 'ball',\n",
       " 'drawings',\n",
       " 'make',\n",
       " 'mlb',\n",
       " 'long',\n",
       " 'amazing',\n",
       " 'nbsp',\n",
       " 'fight',\n",
       " 'jones',\n",
       " 'like',\n",
       " 'jon',\n",
       " 'ufc',\n",
       " 'conor',\n",
       " 'get',\n",
       " 'one',\n",
       " 'shit',\n",
       " 'time',\n",
       " 'fuck',\n",
       " 'even',\n",
       " 'fucking',\n",
       " 'mma',\n",
       " 'https',\n",
       " 'mayweather',\n",
       " 'would',\n",
       " 'mcgregor',\n",
       " 'got',\n",
       " 'think',\n",
       " 'see',\n",
       " 'floyd',\n",
       " 'going',\n",
       " 'man',\n",
       " 'money',\n",
       " 'way',\n",
       " 'know',\n",
       " 'khabib',\n",
       " 'dana',\n",
       " 'guy',\n",
       " 'deleted',\n",
       " 'people',\n",
       " 'really',\n",
       " 'never',\n",
       " 'much',\n",
       " 'take',\n",
       " 'make',\n",
       " 'ever',\n",
       " 'fighter',\n",
       " 'well',\n",
       " 'vs',\n",
       " 'win',\n",
       " 'right',\n",
       " 'still',\n",
       " 'dc',\n",
       " 'boxing',\n",
       " 'na',\n",
       " 'lol',\n",
       " 'back',\n",
       " 'love',\n",
       " 'ryan',\n",
       " 'could',\n",
       " 'life',\n",
       " 'go',\n",
       " 'someone',\n",
       " 'said',\n",
       " 'fighters',\n",
       " 'steroids',\n",
       " 'good',\n",
       " 'thing',\n",
       " 'askren',\n",
       " 'gon',\n",
       " 'actually',\n",
       " 'watch',\n",
       " 'getting',\n",
       " 'hope',\n",
       " 'something',\n",
       " '1',\n",
       " 'hall',\n",
       " 'say',\n",
       " 'everyone',\n",
       " 'beat',\n",
       " 'kick',\n",
       " 'wins',\n",
       " 'let',\n",
       " 'like',\n",
       " 'try',\n",
       " 'rugby',\n",
       " 'would',\n",
       " 'get',\n",
       " 'one',\n",
       " 'forward',\n",
       " 'whale',\n",
       " 'shit',\n",
       " 'man',\n",
       " 'leaf',\n",
       " 'good',\n",
       " 'england',\n",
       " 'great',\n",
       " 'https',\n",
       " 'pass',\n",
       " 'amazing',\n",
       " 'seen',\n",
       " 'know',\n",
       " 'got',\n",
       " 'beluga',\n",
       " 'please',\n",
       " 'well',\n",
       " 'game',\n",
       " 'ball',\n",
       " 'see',\n",
       " 'deleted',\n",
       " 'best',\n",
       " 'back',\n",
       " 'love',\n",
       " 'south',\n",
       " 'fucking',\n",
       " 'japan',\n",
       " 'throwing',\n",
       " 'really',\n",
       " 'play',\n",
       " 'looks',\n",
       " 'last',\n",
       " 'lions',\n",
       " 'look',\n",
       " 'going',\n",
       " 'world',\n",
       " 'win',\n",
       " 'blacks',\n",
       " 'scotland',\n",
       " 'oh',\n",
       " 'year',\n",
       " 'tackle',\n",
       " 'right',\n",
       " 'thought',\n",
       " 'think',\n",
       " 'match',\n",
       " 'go',\n",
       " 'time',\n",
       " 'beautiful',\n",
       " 'still',\n",
       " 'something',\n",
       " 'stop',\n",
       " 'trash',\n",
       " 'whales',\n",
       " 'much',\n",
       " 'absolutely',\n",
       " 'thing',\n",
       " 'take',\n",
       " 'anyone',\n",
       " 'team',\n",
       " 'scottish',\n",
       " 'could',\n",
       " 'brilliant',\n",
       " 'nice',\n",
       " 'people',\n",
       " 'say',\n",
       " 'fetch',\n",
       " 'better',\n",
       " 'keep',\n",
       " 'league',\n",
       " 'like',\n",
       " 'football',\n",
       " 'game',\n",
       " 'fans',\n",
       " 'get',\n",
       " 'chelsea',\n",
       " 'fan',\n",
       " 'one',\n",
       " 'think',\n",
       " 'got',\n",
       " 'even',\n",
       " 'arsenal',\n",
       " 'big',\n",
       " 'would',\n",
       " 'clubs',\n",
       " 'foul',\n",
       " 'refs',\n",
       " 'man',\n",
       " 'every',\n",
       " 'see',\n",
       " 'fuck',\n",
       " 'team',\n",
       " 'var',\n",
       " 'shit',\n",
       " 'could',\n",
       " 'going',\n",
       " 'go',\n",
       " 'fucking',\n",
       " 'back',\n",
       " 'time',\n",
       " 'love',\n",
       " 'play',\n",
       " 'know',\n",
       " 'city',\n",
       " 'good',\n",
       " 'goal',\n",
       " 'liverpool',\n",
       " 'player',\n",
       " 'ball',\n",
       " 'us',\n",
       " '6',\n",
       " 'teams',\n",
       " 'ref',\n",
       " 'da',\n",
       " 'penalty',\n",
       " 'watch',\n",
       " 'premier',\n",
       " 'super',\n",
       " 'thing',\n",
       " 'club',\n",
       " 'well',\n",
       " 'season',\n",
       " 'best',\n",
       " 'red',\n",
       " 'also',\n",
       " 'bias',\n",
       " 'pen',\n",
       " 'need',\n",
       " 'top',\n",
       " 'never',\n",
       " 'andersen',\n",
       " 'players',\n",
       " 'win',\n",
       " 'richarlison',\n",
       " 'united',\n",
       " 'make',\n",
       " 'support',\n",
       " 'tune',\n",
       " 'chant',\n",
       " 'money',\n",
       " 'really',\n",
       " 'nunez',\n",
       " 'let',\n",
       " 'take']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "flat_list = list(chain.from_iterable(SportsFDarray))\n",
    "flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb2dfc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "see        9\n",
      "go         9\n",
      "got        9\n",
      "going      9\n",
      "would      9\n",
      "think      9\n",
      "fucking    9\n",
      "know       9\n",
      "love       9\n",
      "get        9\n",
      "really     9\n",
      "good       9\n",
      "time       9\n",
      "like       9\n",
      "one        9\n",
      "man        9\n",
      "fuck       8\n",
      "could      8\n",
      "never      8\n",
      "shit       8\n",
      "back       8\n",
      "win        7\n",
      "well       7\n",
      "still      7\n",
      "last       7\n",
      "right      7\n",
      "people     7\n",
      "much       7\n",
      "game       7\n",
      "https      7\n",
      "even       7\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "words = pd.Series(flat_list).value_counts()\n",
    "print(words[words >= 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c313f3",
   "metadata": {},
   "source": [
    "### Movies Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50d06112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   movies\n",
      "Original: 0.18186679630529898\n",
      "Processed 0.3153325817361894\n",
      "Processing:   harrypotter\n",
      "Original: 0.15321063944116067\n",
      "Processed 0.2707665169980757\n",
      "Processing:   StarWars\n",
      "Original: 0.1529322345787663\n",
      "Processed 0.2779051987767584\n",
      "Processing:   anime\n",
      "Original: 0.15819171534784918\n",
      "Processed 0.25487568855143666\n",
      "Processing:   batman\n",
      "Original: 0.1422777965528895\n",
      "Processed 0.2551519525298693\n",
      "Processing:   moviecritic\n",
      "Original: 0.1060315021129466\n",
      "Processed 0.18851602794280864\n",
      "Processing:   MovieSuggestions\n",
      "Original: 0.1757307856837302\n",
      "Processed 0.29219505817753955\n",
      "Processing:   Hungergames\n",
      "Original: 0.3586626139817629\n",
      "Processed 0.6135135135135135\n",
      "Processing:   MarvelStudiosSpoilers\n",
      "Original: 0.16895929416482855\n",
      "Processed 0.2776285317276517\n",
      "Average Orignal Score is:    0.177540375352137\n",
      "Average Processed Score is:    0.30509834110598255\n",
      "Min Orignal Score is:    0.1060315021129466\n",
      "Min Processed Score is:    0.18851602794280864\n",
      "Max Orignal Score is:    0.3586626139817629\n",
      "Max Processed Score is:    0.6135135135135135\n",
      "CPU times: total: 36.5 s\n",
      "Wall time: 39.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "know       9\n",
       "get        9\n",
       "really     9\n",
       "got        9\n",
       "way        9\n",
       "even       9\n",
       "would      9\n",
       "love       9\n",
       "time       9\n",
       "like       9\n",
       "one        9\n",
       "people     8\n",
       "also       8\n",
       "good       8\n",
       "still      8\n",
       "think      8\n",
       "first      8\n",
       "see        8\n",
       "much       8\n",
       "well       8\n",
       "made       8\n",
       "great      8\n",
       "fucking    7\n",
       "ever       7\n",
       "man        7\n",
       "never      7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "recordOriginal = []\n",
    "recordPros =[]\n",
    "\n",
    "MoviesFDarray = []\n",
    "\n",
    "for i in MoviesSubs:\n",
    "    print(\"Processing:  \", i)\n",
    "    ulysses_txt = load_txt(f'{i}.txt')\n",
    "    ulysses_tokens = nltk.word_tokenize(ulysses_txt)\n",
    "\n",
    "\n",
    "\n",
    "    ulysses_lowercase = [t.lower() for t in ulysses_tokens]\n",
    "    ulysses_no_punct = [t for t in ulysses_lowercase if t.isalnum()]\n",
    "    ulysses_no_stop = [t for t in ulysses_no_punct if t not in stopwords.words('english')]\n",
    "\n",
    "    orig = lexical_diversity(ulysses_tokens)\n",
    "    pros = lexical_diversity(ulysses_no_stop)\n",
    "    print(\"Original:\", orig)\n",
    "    print(\"Processed\", pros)\n",
    "    \n",
    "    recordOriginal.append(orig)\n",
    "    recordPros.append(pros)\n",
    "\n",
    "    fdist1 = nltk.FreqDist(ulysses_no_stop)\n",
    "    top_75 = fdist1.most_common(75)\n",
    "    \n",
    "    top_words =[]\n",
    "    for i in top_75:\n",
    "        top_words.append(i[0])\n",
    "    \n",
    "    MoviesFDarray.append(top_words)\n",
    "\n",
    "flat_list = list(chain.from_iterable(MoviesFDarray))\n",
    "words = pd.Series(flat_list).value_counts()\n",
    "\n",
    "Movie_common_to_mostsubs = words[words >= (len(MoviesSubs)-2)]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"Average Orignal Score is:   \", statistics.mean(recordOriginal))\n",
    "print(\"Average Processed Score is:   \", statistics.mean(recordPros))\n",
    "\n",
    "print(\"Min Orignal Score is:   \", min(recordOriginal))\n",
    "print(\"Min Processed Score is:   \", min(recordPros))\n",
    "\n",
    "print(\"Max Orignal Score is:   \", max(recordOriginal))\n",
    "print(\"Max Processed Score is:   \", max(recordPros))\n",
    "\n",
    "Movie_common_to_mostsubs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c834d72c",
   "metadata": {},
   "source": [
    "### Learning Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "722ee668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   todayilearned\n",
      "Original: 0.1856177012907191\n",
      "Processed 0.3619377936898635\n",
      "Processing:   science\n",
      "Original: 0.1810369251109642\n",
      "Processed 0.33894019349164467\n",
      "Processing:   askscience\n",
      "Original: 0.14548189799743513\n",
      "Processed 0.26618870088901636\n",
      "Processing:   space\n",
      "Original: 0.15037770100134684\n",
      "Processed 0.2711407491486947\n",
      "Processing:   AskHistorians\n",
      "Original: 0.13666784645580618\n",
      "Processed 0.2576911589008363\n",
      "Processing:   YouShouldKnow\n",
      "Original: 0.10364215563939576\n",
      "Processed 0.1970611096716882\n",
      "Processing:   explainlikeimfive\n",
      "Original: 0.10209126555662897\n",
      "Processed 0.19309858208453734\n",
      "Average Orignal Score is:    0.14355935615032803\n",
      "Average Processed Score is:    0.2694368982680402\n",
      "Min Orignal Score is:    0.10209126555662897\n",
      "Min Processed Score is:    0.19309858208453734\n",
      "Max Orignal Score is:    0.1856177012907191\n",
      "Max Processed Score is:    0.3619377936898635\n",
      "CPU times: total: 45 s\n",
      "Wall time: 46.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "would        7\n",
       "really       7\n",
       "know         7\n",
       "way          7\n",
       "years        7\n",
       "see          7\n",
       "even         7\n",
       "much         7\n",
       "people       7\n",
       "good         7\n",
       "make         7\n",
       "think        7\n",
       "back         7\n",
       "get          7\n",
       "could        7\n",
       "well         7\n",
       "like         7\n",
       "one          7\n",
       "time         7\n",
       "us           7\n",
       "still        6\n",
       "say          6\n",
       "many         6\n",
       "go           6\n",
       "want         6\n",
       "work         6\n",
       "also         6\n",
       "something    6\n",
       "every        5\n",
       "removed      5\n",
       "https        5\n",
       "never        5\n",
       "first        5\n",
       "going        5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "recordOriginal = []\n",
    "recordPros =[]\n",
    "\n",
    "LearningFDarray = []\n",
    "\n",
    "for i in LearningSubs:\n",
    "    print(\"Processing:  \", i)\n",
    "    ulysses_txt = load_txt(f'{i}.txt')\n",
    "    ulysses_tokens = nltk.word_tokenize(ulysses_txt)\n",
    "\n",
    "\n",
    "\n",
    "    ulysses_lowercase = [t.lower() for t in ulysses_tokens]\n",
    "    ulysses_no_punct = [t for t in ulysses_lowercase if t.isalnum()]\n",
    "    ulysses_no_stop = [t for t in ulysses_no_punct if t not in stopwords.words('english')]\n",
    "\n",
    "    orig = lexical_diversity(ulysses_tokens)\n",
    "    pros = lexical_diversity(ulysses_no_stop)\n",
    "    print(\"Original:\", orig)\n",
    "    print(\"Processed\", pros)\n",
    "    \n",
    "    recordOriginal.append(orig)\n",
    "    recordPros.append(pros)\n",
    "\n",
    "    fdist1 = nltk.FreqDist(ulysses_no_stop)\n",
    "    top_75 = fdist1.most_common(75)\n",
    "    \n",
    "    top_words =[]\n",
    "    for i in top_75:\n",
    "        top_words.append(i[0])\n",
    "    \n",
    "    LearningFDarray.append(top_words)\n",
    "    \n",
    "flat_list = list(chain.from_iterable(LearningFDarray))\n",
    "words = pd.Series(flat_list).value_counts()\n",
    "\n",
    "Learning_common_to_mostsubs = words[words >= (len(LearningSubs)-2)]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"Average Orignal Score is:   \", statistics.mean(recordOriginal))\n",
    "print(\"Average Processed Score is:   \", statistics.mean(recordPros))\n",
    "\n",
    "print(\"Min Orignal Score is:   \", min(recordOriginal))\n",
    "print(\"Min Processed Score is:   \", min(recordPros))\n",
    "\n",
    "print(\"Max Orignal Score is:   \", max(recordOriginal))\n",
    "print(\"Max Processed Score is:   \", max(recordPros))\n",
    "\n",
    "Learning_common_to_mostsubs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156586b",
   "metadata": {},
   "source": [
    "### News and Politics Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "80c6257b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   politics\n",
      "Original: 0.13826141430519218\n",
      "Processed 0.23071634222852477\n",
      "Processing:   worldnews\n",
      "Original: 0.18405618964003512\n",
      "Processed 0.3325020112630732\n",
      "Processing:   news\n",
      "Original: 0.18434713691301763\n",
      "Processed 0.33449605287314116\n",
      "Processing:   conspiracy\n",
      "Original: 0.15171252603128185\n",
      "Processed 0.27811533951604306\n",
      "Processing:   Libertarian\n",
      "Original: 0.14108324878001643\n",
      "Processed 0.26883600270697044\n",
      "Processing:   offbeat\n",
      "Original: 0.18122452060999547\n",
      "Processed 0.33085370173574213\n",
      "Processing:   TrueReddit\n",
      "Original: 0.17563782269664624\n",
      "Processed 0.3391991380858323\n",
      "Processing:   Conservative\n",
      "Original: 0.1540825569358178\n",
      "Processed 0.2895554040748315\n",
      "Average Orignal Score is:    0.16380067698900033\n",
      "Average Processed Score is:    0.30053424906051984\n",
      "Min Orignal Score is:    0.13826141430519218\n",
      "Min Processed Score is:    0.23071634222852477\n",
      "Max Orignal Score is:    0.18434713691301763\n",
      "Max Processed Score is:    0.3391991380858323\n",
      "CPU times: total: 38.6 s\n",
      "Wall time: 40.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trump      8\n",
       "one        8\n",
       "going      8\n",
       "good       8\n",
       "know       8\n",
       "way        8\n",
       "even       8\n",
       "really     8\n",
       "https      8\n",
       "time       8\n",
       "right      8\n",
       "think      8\n",
       "would      8\n",
       "like       8\n",
       "much       8\n",
       "get        8\n",
       "want       8\n",
       "people     8\n",
       "well       8\n",
       "could      8\n",
       "see        8\n",
       "make       7\n",
       "say        7\n",
       "also       7\n",
       "go         7\n",
       "deleted    7\n",
       "still      7\n",
       "got        7\n",
       "years      7\n",
       "never      7\n",
       "us         7\n",
       "fuck       7\n",
       "shit       6\n",
       "fucking    6\n",
       "every      6\n",
       "take       6\n",
       "need       6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "recordOriginal = []\n",
    "recordPros =[]\n",
    "\n",
    "NewsFDarray = []\n",
    "\n",
    "for i in NewsSubs:\n",
    "    print(\"Processing:  \", i)\n",
    "    ulysses_txt = load_txt(f'{i}.txt')\n",
    "    ulysses_tokens = nltk.word_tokenize(ulysses_txt)\n",
    "\n",
    "\n",
    "\n",
    "    ulysses_lowercase = [t.lower() for t in ulysses_tokens]\n",
    "    ulysses_no_punct = [t for t in ulysses_lowercase if t.isalnum()]\n",
    "    ulysses_no_stop = [t for t in ulysses_no_punct if t not in stopwords.words('english')]\n",
    "\n",
    "    orig = lexical_diversity(ulysses_tokens)\n",
    "    pros = lexical_diversity(ulysses_no_stop)\n",
    "    print(\"Original:\", orig)\n",
    "    print(\"Processed\", pros)\n",
    "    \n",
    "    recordOriginal.append(orig)\n",
    "    recordPros.append(pros)\n",
    "\n",
    "    fdist1 = nltk.FreqDist(ulysses_no_stop)\n",
    "    top_75 = fdist1.most_common(75)\n",
    "    \n",
    "    top_words =[]\n",
    "    for i in top_75:\n",
    "        top_words.append(i[0])\n",
    "    \n",
    "    NewsFDarray.append(top_words)\n",
    "    \n",
    "flat_list = list(chain.from_iterable(NewsFDarray))\n",
    "words = pd.Series(flat_list).value_counts()\n",
    "\n",
    "News_common_to_mostsubs = words[words >= (len(NewsSubs)-2)]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"Average Orignal Score is:   \", statistics.mean(recordOriginal))\n",
    "print(\"Average Processed Score is:   \", statistics.mean(recordPros))\n",
    "\n",
    "print(\"Min Orignal Score is:   \", min(recordOriginal))\n",
    "print(\"Min Processed Score is:   \", min(recordPros))\n",
    "\n",
    "print(\"Max Orignal Score is:   \", max(recordOriginal))\n",
    "print(\"Max Processed Score is:   \", max(recordPros))\n",
    "\n",
    "News_common_to_mostsubs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f274ada9",
   "metadata": {},
   "source": [
    "### TV Series Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9f36320b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   arresteddevelopment\n",
      "Original: 0.18823219691983678\n",
      "Processed 0.34176424668227945\n",
      "Processing:   gameofthrones\n",
      "Original: 0.20014830025096966\n",
      "Processed 0.34968698096333206\n",
      "Processing:   doctorwho\n",
      "Original: 0.13345221317840605\n",
      "Processed 0.2462155249879035\n",
      "Processing:   mylittlepony\n",
      "Original: 0.13651560926485398\n",
      "Processed 0.266895884817879\n",
      "Processing:   community\n",
      "Original: 0.17472852912142153\n",
      "Processed 0.3123844731977819\n",
      "Processing:   breakingbad\n",
      "Original: 0.1932949188056574\n",
      "Processed 0.33228980322003576\n",
      "Processing:   adventuretime\n",
      "Original: 0.21006227695752572\n",
      "Processed 0.3595206391478029\n",
      "Average Orignal Score is:    0.1766334349283816\n",
      "Average Processed Score is:    0.31553679328814493\n",
      "Min Orignal Score is:    0.13345221317840605\n",
      "Min Processed Score is:    0.2462155249879035\n",
      "Max Orignal Score is:    0.21006227695752572\n",
      "Max Processed Score is:    0.3595206391478029\n",
      "CPU times: total: 20.2 s\n",
      "Wall time: 21.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "going      7\n",
       "see        7\n",
       "well       7\n",
       "never      7\n",
       "would      7\n",
       "back       7\n",
       "made       7\n",
       "know       7\n",
       "good       7\n",
       "people     7\n",
       "love       7\n",
       "could      7\n",
       "best       7\n",
       "really     7\n",
       "make       7\n",
       "think      7\n",
       "show       7\n",
       "one        7\n",
       "get        7\n",
       "time       7\n",
       "like       7\n",
       "got        6\n",
       "right      6\n",
       "episode    6\n",
       "great      6\n",
       "https      6\n",
       "watch      6\n",
       "much       6\n",
       "even       6\n",
       "still      6\n",
       "first      6\n",
       "want       5\n",
       "amazing    5\n",
       "looks      5\n",
       "ever       5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "recordOriginal = []\n",
    "recordPros =[]\n",
    "\n",
    "TVFDarray = []\n",
    "\n",
    "for i in TVSubs:\n",
    "    print(\"Processing:  \", i)\n",
    "    ulysses_txt = load_txt(f'{i}.txt')\n",
    "    ulysses_tokens = nltk.word_tokenize(ulysses_txt)\n",
    "\n",
    "\n",
    "\n",
    "    ulysses_lowercase = [t.lower() for t in ulysses_tokens]\n",
    "    ulysses_no_punct = [t for t in ulysses_lowercase if t.isalnum()]\n",
    "    ulysses_no_stop = [t for t in ulysses_no_punct if t not in stopwords.words('english')]\n",
    "\n",
    "    orig = lexical_diversity(ulysses_tokens)\n",
    "    pros = lexical_diversity(ulysses_no_stop)\n",
    "    print(\"Original:\", orig)\n",
    "    print(\"Processed\", pros)\n",
    "    \n",
    "    recordOriginal.append(orig)\n",
    "    recordPros.append(pros)\n",
    "\n",
    "    fdist1 = nltk.FreqDist(ulysses_no_stop)\n",
    "    top_75 = fdist1.most_common(75)\n",
    "    \n",
    "    top_words =[]\n",
    "    for i in top_75:\n",
    "        top_words.append(i[0])\n",
    "    \n",
    "    TVFDarray.append(top_words)\n",
    "    \n",
    "flat_list = list(chain.from_iterable(TVFDarray))\n",
    "words = pd.Series(flat_list).value_counts()\n",
    "\n",
    "TV_common_to_mostsubs = words[words >= (len(TVSubs)-2)]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"Average Orignal Score is:   \", statistics.mean(recordOriginal))\n",
    "print(\"Average Processed Score is:   \", statistics.mean(recordPros))\n",
    "\n",
    "print(\"Min Orignal Score is:   \", min(recordOriginal))\n",
    "print(\"Min Processed Score is:   \", min(recordPros))\n",
    "\n",
    "print(\"Max Orignal Score is:   \", max(recordOriginal))\n",
    "print(\"Max Processed Score is:   \", max(recordPros))\n",
    "\n",
    "TV_common_to_mostsubs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94694b68",
   "metadata": {},
   "source": [
    "### Gaming Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ffbb5f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   gaming\n",
      "Original: 0.1796878798191453\n",
      "Processed 0.33046311380032584\n",
      "Processing:   leagueoflegends\n",
      "Original: 0.1304221174725915\n",
      "Processed 0.1772517886896011\n",
      "Processing:   pokemon\n",
      "Original: 0.16491904394757131\n",
      "Processed 0.2766506495817761\n",
      "Processing:   Minecraft\n",
      "Original: 0.1432326313398449\n",
      "Processed 0.2354803091645197\n",
      "Processing:   starcraft\n",
      "Original: 0.1193025641025641\n",
      "Processed 0.21206404488946937\n",
      "Processing:   Games\n",
      "Original: 0.14647283856159143\n",
      "Processed 0.2683872787532614\n",
      "Processing:   DotA2\n",
      "Original: 0.17468072340790264\n",
      "Processed 0.27434982151963283\n",
      "Processing:   skyrim\n",
      "Original: 0.16707897829179294\n",
      "Processed 0.28329914408997664\n",
      "Processing:   tf2\n",
      "Original: 0.15462778882261985\n",
      "Processed 0.2554483744194355\n",
      "Processing:   magicTCG\n",
      "Original: 0.13860778902177245\n",
      "Processed 0.2599498027967013\n",
      "Processing:   wow\n",
      "Original: 0.1173008678542567\n",
      "Processed 0.2109763617677287\n",
      "Processing:   KerbalSpaceProgram\n",
      "Original: 0.18944034671000543\n",
      "Processed 0.32682687591956844\n",
      "Processing:   mindcrack\n",
      "Original: 0.11336334263069234\n",
      "Processed 0.21790184221628464\n",
      "Average Orignal Score is:    0.14916437784479622\n",
      "Average Processed Score is:    0.2560807236621755\n",
      "Min Orignal Score is:    0.11336334263069234\n",
      "Min Processed Score is:    0.1772517886896011\n",
      "Max Orignal Score is:    0.18944034671000543\n",
      "Max Processed Score is:    0.33046311380032584\n",
      "CPU times: total: 58.7 s\n",
      "Wall time: 1min 7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "like       13\n",
       "one        13\n",
       "really     13\n",
       "get        13\n",
       "time       13\n",
       "know       13\n",
       "would      13\n",
       "good       13\n",
       "make       13\n",
       "going      13\n",
       "got        13\n",
       "think      12\n",
       "love       12\n",
       "much       12\n",
       "never      12\n",
       "game       12\n",
       "people     12\n",
       "see        12\n",
       "even       12\n",
       "made       11\n",
       "could      11\n",
       "fucking    11\n",
       "way        11\n",
       "well       11\n",
       "still      11\n",
       "first      11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "recordOriginal = []\n",
    "recordPros =[]\n",
    "\n",
    "GameFDarray = []\n",
    "\n",
    "for i in GamingSubs:\n",
    "    print(\"Processing:  \", i)\n",
    "    ulysses_txt = load_txt(f'{i}.txt')\n",
    "    ulysses_tokens = nltk.word_tokenize(ulysses_txt)\n",
    "\n",
    "\n",
    "\n",
    "    ulysses_lowercase = [t.lower() for t in ulysses_tokens]\n",
    "    ulysses_no_punct = [t for t in ulysses_lowercase if t.isalnum()]\n",
    "    ulysses_no_stop = [t for t in ulysses_no_punct if t not in stopwords.words('english')]\n",
    "\n",
    "    orig = lexical_diversity(ulysses_tokens)\n",
    "    pros = lexical_diversity(ulysses_no_stop)\n",
    "    print(\"Original:\", orig)\n",
    "    print(\"Processed\", pros)\n",
    "    \n",
    "    recordOriginal.append(orig)\n",
    "    recordPros.append(pros)\n",
    "\n",
    "    fdist1 = nltk.FreqDist(ulysses_no_stop)\n",
    "    top_75 = fdist1.most_common(75)\n",
    "    \n",
    "    top_words =[]\n",
    "    for i in top_75:\n",
    "        top_words.append(i[0])\n",
    "    \n",
    "    GameFDarray.append(top_words)\n",
    "    \n",
    "flat_list = list(chain.from_iterable(GameFDarray))\n",
    "words = pd.Series(flat_list).value_counts()\n",
    "\n",
    "Gaming_common_to_mostsubs = words[words >= (len(GamingSubs)-2)]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"Average Orignal Score is:   \", statistics.mean(recordOriginal))\n",
    "print(\"Average Processed Score is:   \", statistics.mean(recordPros))\n",
    "\n",
    "print(\"Min Orignal Score is:   \", min(recordOriginal))\n",
    "print(\"Min Processed Score is:   \", min(recordPros))\n",
    "\n",
    "print(\"Max Orignal Score is:   \", max(recordOriginal))\n",
    "print(\"Max Processed Score is:   \", max(recordPros))\n",
    "\n",
    "Gaming_common_to_mostsubs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7399cd",
   "metadata": {},
   "source": [
    "### Humour Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "155f4e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   funny\n",
      "Original: 0.19205722216291235\n",
      "Processed 0.35022635814889336\n",
      "Processing:   fffffffuuuuuuuuuuuu\n",
      "Original: 0.219676333920846\n",
      "Processed 0.3985603333964766\n",
      "Processing:   4chan\n",
      "Original: 0.26633208574345013\n",
      "Processed 0.4661369655694287\n",
      "Processing:   lmGoingToHellForThis\n",
      "Original: 0.39334548769371014\n",
      "Processed 0.6790123456790124\n",
      "Processing:   firstworldanarchists\n",
      "Original: 0.21344943225275048\n",
      "Processed 0.3841040035455754\n",
      "Processing:   circlejerk\n",
      "Original: 0.16948214457599384\n",
      "Processed 0.2524679689140937\n",
      "Processing:   okbuddyretard\n",
      "Original: 0.3688751244331379\n",
      "Processed 0.39171052631578945\n",
      "Processing:   facepalm\n",
      "Original: 0.1799339618888491\n",
      "Processed 0.34156649135987976\n",
      "Processing:   Jokes\n",
      "Original: 0.17389784593874252\n",
      "Processed 0.3012632642748863\n",
      "Average Orignal Score is:    0.2418944042900436\n",
      "Average Processed Score is:    0.3961164730226706\n",
      "Min Orignal Score is:    0.16948214457599384\n",
      "Min Processed Score is:    0.2524679689140937\n",
      "Max Orignal Score is:    0.39334548769371014\n",
      "Max Processed Score is:    0.6790123456790124\n",
      "CPU times: total: 22.6 s\n",
      "Wall time: 23.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "like        9\n",
       "one         9\n",
       "get         9\n",
       "would       9\n",
       "see         9\n",
       "time        9\n",
       "actually    8\n",
       "know        8\n",
       "think       8\n",
       "post        8\n",
       "even        8\n",
       "deleted     8\n",
       "shit        8\n",
       "https       8\n",
       "people      8\n",
       "right       8\n",
       "got         8\n",
       "make        8\n",
       "good        8\n",
       "reddit      7\n",
       "fuck        7\n",
       "really      7\n",
       "well        7\n",
       "want        7\n",
       "much        7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "recordOriginal = []\n",
    "recordPros =[]\n",
    "\n",
    "HumourFDarray = []\n",
    "\n",
    "for i in HumourSubs:\n",
    "    print(\"Processing:  \", i)\n",
    "    ulysses_txt = load_txt(f'{i}.txt')\n",
    "    ulysses_tokens = nltk.word_tokenize(ulysses_txt)\n",
    "\n",
    "\n",
    "\n",
    "    ulysses_lowercase = [t.lower() for t in ulysses_tokens]\n",
    "    ulysses_no_punct = [t for t in ulysses_lowercase if t.isalnum()]\n",
    "    ulysses_no_stop = [t for t in ulysses_no_punct if t not in stopwords.words('english')]\n",
    "\n",
    "    orig = lexical_diversity(ulysses_tokens)\n",
    "    pros = lexical_diversity(ulysses_no_stop)\n",
    "    print(\"Original:\", orig)\n",
    "    print(\"Processed\", pros)\n",
    "    \n",
    "    recordOriginal.append(orig)\n",
    "    recordPros.append(pros)\n",
    "\n",
    "    fdist1 = nltk.FreqDist(ulysses_no_stop)\n",
    "    top_75 = fdist1.most_common(75)\n",
    "    \n",
    "    top_words =[]\n",
    "    for i in top_75:\n",
    "        top_words.append(i[0])\n",
    "    \n",
    "    HumourFDarray.append(top_words)\n",
    "    \n",
    "flat_list = list(chain.from_iterable(HumourFDarray))\n",
    "words = pd.Series(flat_list).value_counts()\n",
    "\n",
    "Humour_common_to_mostsubs = words[words >= (len(HumourSubs)-2)]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"Average Orignal Score is:   \", statistics.mean(recordOriginal))\n",
    "print(\"Average Processed Score is:   \", statistics.mean(recordPros))\n",
    "\n",
    "print(\"Min Orignal Score is:   \", min(recordOriginal))\n",
    "print(\"Min Processed Score is:   \", min(recordPros))\n",
    "\n",
    "print(\"Max Orignal Score is:   \", max(recordOriginal))\n",
    "print(\"Max Processed Score is:   \", max(recordPros))\n",
    "\n",
    "Humour_common_to_mostsubs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e9e41c",
   "metadata": {},
   "source": [
    "### Discussion Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84d7391d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   CrazyIdeas\n",
      "Original: 0.17933967996759165\n",
      "Processed 0.31565537416249884\n",
      "Processing:   AskReddit\n",
      "Original: 0.14087286902030444\n",
      "Processed 0.26803768382352944\n",
      "Processing:   fatpeoplestories\n",
      "Original: 0.181605702933271\n",
      "Processed 0.33182844243792325\n",
      "Processing:   DoesAnybodyElse\n",
      "Original: 0.12351633209107138\n",
      "Processed 0.23988799004355943\n",
      "Processing:   IAmA\n",
      "Original: 0.15452163648403536\n",
      "Processed 0.29060834659838053\n",
      "Processing:   bestof\n",
      "Original: 0.16513459292750846\n",
      "Processed 0.29750692520775623\n",
      "Processing:   TalesFromRetail\n",
      "Original: 0.13358315488936473\n",
      "Processed 0.2582869265446831\n",
      "Average Orignal Score is:    0.15408199547330673\n",
      "Average Processed Score is:    0.2859730984026187\n",
      "Min Orignal Score is:    0.12351633209107138\n",
      "Min Processed Score is:    0.23988799004355943\n",
      "Max Orignal Score is:    0.181605702933271\n",
      "Max Processed Score is:    0.33182844243792325\n",
      "CPU times: total: 29.6 s\n",
      "Wall time: 30.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "go           7\n",
       "thing        7\n",
       "time         7\n",
       "much         7\n",
       "could        7\n",
       "way          7\n",
       "would        7\n",
       "even         7\n",
       "think        7\n",
       "know         7\n",
       "really       7\n",
       "see          7\n",
       "people       7\n",
       "one          7\n",
       "like         7\n",
       "get          7\n",
       "going        6\n",
       "work         6\n",
       "never        6\n",
       "also         6\n",
       "something    6\n",
       "good         6\n",
       "need         6\n",
       "make         6\n",
       "day          5\n",
       "someone      5\n",
       "back         5\n",
       "feel         5\n",
       "got          5\n",
       "right        5\n",
       "well         5\n",
       "first        5\n",
       "reddit       5\n",
       "want         5\n",
       "still        5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "recordOriginal = []\n",
    "recordPros =[]\n",
    "\n",
    "DiscussFDarray = []\n",
    "\n",
    "for i in DiscussionSubs:\n",
    "    print(\"Processing:  \", i)\n",
    "    ulysses_txt = load_txt(f'{i}.txt')\n",
    "    ulysses_tokens = nltk.word_tokenize(ulysses_txt)\n",
    "\n",
    "\n",
    "\n",
    "    ulysses_lowercase = [t.lower() for t in ulysses_tokens]\n",
    "    ulysses_no_punct = [t for t in ulysses_lowercase if t.isalnum()]\n",
    "    ulysses_no_stop = [t for t in ulysses_no_punct if t not in stopwords.words('english')]\n",
    "\n",
    "    orig = lexical_diversity(ulysses_tokens)\n",
    "    pros = lexical_diversity(ulysses_no_stop)\n",
    "    print(\"Original:\", orig)\n",
    "    print(\"Processed\", pros)\n",
    "    \n",
    "    recordOriginal.append(orig)\n",
    "    recordPros.append(pros)\n",
    "\n",
    "    fdist1 = nltk.FreqDist(ulysses_no_stop)\n",
    "    top_75 = fdist1.most_common(75)\n",
    "    \n",
    "    top_words =[]\n",
    "    for i in top_75:\n",
    "        top_words.append(i[0])\n",
    "    \n",
    "    DiscussFDarray.append(top_words)\n",
    "    \n",
    "flat_list = list(chain.from_iterable(DiscussFDarray))\n",
    "words = pd.Series(flat_list).value_counts()\n",
    "\n",
    "Discuss_common_to_mostsubs = words[words >= (len(DiscussionSubs)-2)]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"Average Orignal Score is:   \", statistics.mean(recordOriginal))\n",
    "print(\"Average Processed Score is:   \", statistics.mean(recordPros))\n",
    "\n",
    "print(\"Min Orignal Score is:   \", min(recordOriginal))\n",
    "print(\"Min Processed Score is:   \", min(recordPros))\n",
    "\n",
    "print(\"Max Orignal Score is:   \", max(recordOriginal))\n",
    "print(\"Max Processed Score is:   \", max(recordPros))\n",
    "\n",
    "Discuss_common_to_mostsubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa08e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa33e78c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47750320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cefc17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a9c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffde441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8674e551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d448f577",
   "metadata": {},
   "source": [
    "#### n grams example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bb3e899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   nba\n",
      "\n",
      "\n",
      "3GRAMS:\n",
      "\n",
      "\n",
      "[(('one', 'of', 'the'), 26), (('of', 'all', 'time'), 21), (('this', 'is', 'the'), 20), (('what', 'the', 'fuck'), 17), (('i', 'can', 't'), 15), (('a', 'lot', 'of'), 13), (('is', 'going', 'to'), 13), (('and', 'his', 'daughter'), 11), (('rest', 'in', 'peace'), 11), (('post', 'of', 'all'), 11), (('in', 'the', 'crash'), 10), (('to', 'be', 'the'), 10), (('gon', 'na', 'be'), 10), (('a', 'helicopter', 'crash'), 9), (('in', 'nba', 'history'), 9), (('i', 'don', 't'), 9), (('this', 'is', 'just'), 9), (('hall', 'of', 'fame'), 9), (('the', 'rest', 'of'), 9), (('i', 'want', 'to'), 9), (('on', 'board', 'the'), 8), (('died', 'in', 'the'), 8), (('going', 'to', 'be'), 8), (('part', 'of', 'the'), 8), (('is', 'gon', 'na'), 8)]\n",
      "\n",
      "\n",
      "NO STOP WORDS:\n",
      "\n",
      "\n",
      "[(('kobe', 'daughter', 'gianna'), 5), (('gigi', 'also', 'board'), 5), (('board', 'helicopter', 'died'), 5), (('helicopter', 'died', 'crash'), 5), (('top', 'post', 'time'), 5), (('hall', 'fame', 'speech'), 5), (('13', 'year', 'old'), 5), (('strip', 'club', 'rating'), 5), (('kobe', 'bryant', 'died'), 4), (('daughter', 'gianna', 'maria'), 4), (('aka', 'gigi', 'also'), 4), (('also', 'board', 'helicopter'), 4), (('died', 'crash', 'reps'), 4), (('crash', 'reps', 'kobe'), 4), (('reps', 'kobe', 'tell'), 4), (('kobe', 'tell', 'tmz'), 4), (('tell', 'tmz', 'sports'), 4), (('kobe', 'bryant', 'passed'), 4), (('bryant', 'passed', 'away'), 4), (('someone', 'never', 'met'), 4), (('still', 'ca', 'believe'), 4), (('rip', 'kobe', 'bryant'), 4), (('worst', 'strip', 'clubs'), 4), (('another', 'player', 'parent'), 4), (('helicopter', 'crash', 'calabasas'), 3)]\n",
      "\n",
      "\n",
      "2GRAMS:\n",
      "\n",
      "\n",
      "[(('this', 'is'), 121), (('of', 'the'), 105), (('in', 'the'), 103), (('to', 'be'), 69), (('to', 'the'), 59), (('he', 'was'), 59), (('i', 'was'), 52), (('on', 'the'), 47), (('for', 'the'), 45), (('and', 'i'), 44), (('i', 'm'), 44), (('the', 'nba'), 42), (('one', 'of'), 41), (('and', 'the'), 41), (('kobe', 'bryant'), 40), (('is', 'the'), 39), (('as', 'a'), 39), (('his', 'daughter'), 38), (('but', 'i'), 35), (('i', 'can'), 34), (('all', 'time'), 34), (('i', 'just'), 33), (('going', 'to'), 33), (('in', 'a'), 32), (('on', 'board'), 31)]\n",
      "\n",
      "\n",
      "NO STOP WORDS:\n",
      "\n",
      "\n",
      "[(('kobe', 'bryant'), 40), (('gon', 'na'), 24), (('strip', 'clubs'), 23), (('rip', 'kobe'), 16), (('daughter', 'gianna'), 13), (('helicopter', 'crash'), 12), (('holy', 'shit'), 12), (('ca', 'believe'), 12), (('celebrity', 'death'), 12), (('post', 'time'), 12), (('kobe', 'daughter'), 11), (('rest', 'peace'), 11), (('never', 'got'), 11), (('passed', 'away'), 10), (('years', 'old'), 10), (('year', 'old'), 10), (('top', 'post'), 10), (('max', 'kellerman'), 10), (('charles', 'barkley'), 10), (('love', 'chuck'), 10), (('died', 'crash'), 9), (('nba', 'history'), 9), (('hall', 'fame'), 9), (('role', 'model'), 9), (('james', 'harden'), 9)]\n"
     ]
    }
   ],
   "source": [
    "i = SportsSubs[0]\n",
    "\n",
    "print(\"Processing:  \", i)\n",
    "ulysses_txt = load_txt(f'{i}.txt')\n",
    "ulysses_tokens = nltk.word_tokenize(ulysses_txt)\n",
    "\n",
    "ulysses_lowercase = [t.lower() for t in ulysses_tokens]\n",
    "ulysses_no_punct = [t for t in ulysses_lowercase if t.isalnum()]\n",
    "ulysses_no_stop = [t for t in ulysses_no_punct if t not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"3GRAMS:\")\n",
    "print(\"\\n\")\n",
    "\n",
    "c3stop = Counter(ngrams(ulysses_no_stop, n=3))\n",
    "c3pun = Counter(ngrams(ulysses_no_punct, n=3))\n",
    "\n",
    "print(c3pun.most_common(25))\n",
    "print(\"\\n\")\n",
    "print(\"NO STOP WORDS:\")\n",
    "print(\"\\n\")\n",
    "print(c3stop.most_common(25))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"2GRAMS:\")\n",
    "print(\"\\n\")\n",
    "\n",
    "c2stop = Counter(ngrams(ulysses_no_stop, n=2))\n",
    "c2pun = Counter(ngrams(ulysses_no_punct, n=2))\n",
    "\n",
    "print(c2pun.most_common(25))\n",
    "print(\"\\n\")\n",
    "print(\"NO STOP WORDS:\")\n",
    "print(\"\\n\")\n",
    "print(c2stop.most_common(25))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b602df",
   "metadata": {},
   "source": [
    "#### n grams group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef201032",
   "metadata": {},
   "source": [
    "### Sports Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e167aec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   nba\n",
      "Processing:   soccer\n",
      "Processing:   hockey\n",
      "Processing:   nfl\n",
      "Processing:   formula1\n",
      "Processing:   baseball\n",
      "Processing:   MMA\n",
      "Processing:   rugbyunion\n",
      "Processing:   PremierLeague\n",
      "CPU times: total: 36.9 s\n",
      "Wall time: 38.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "g3stop = []\n",
    "g3pun = []\n",
    "g2stop = []\n",
    "g2pun = []\n",
    "\n",
    "\n",
    "for i in SportsSubs:\n",
    "    print(\"Processing:  \", i)\n",
    "    ulysses_txt = load_txt(f'{i}.txt')\n",
    "    ulysses_tokens = nltk.word_tokenize(ulysses_txt)\n",
    "\n",
    "    ulysses_lowercase = [t.lower() for t in ulysses_tokens]\n",
    "    ulysses_no_punct = [t for t in ulysses_lowercase if t.isalnum()]\n",
    "    ulysses_no_stop = [t for t in ulysses_no_punct if t not in stopwords.words('english')]\n",
    "    \n",
    "    c3stop = Counter(ngrams(ulysses_no_stop, n=3))\n",
    "    c3pun = Counter(ngrams(ulysses_no_punct, n=3))\n",
    "    \n",
    "    c2stop = Counter(ngrams(ulysses_no_stop, n=2))\n",
    "    c2pun = Counter(ngrams(ulysses_no_punct, n=2))\n",
    "    \n",
    "    \n",
    "    top_3grams_nostop = c3stop.most_common(100)    \n",
    "    top_3grams_nopun = c3pun.most_common(100)\n",
    "    \n",
    "    top_2grams_nostop = c2stop.most_common(100)    \n",
    "    top_2grams_nopun = c2pun.most_common(100)\n",
    "    \n",
    "    grams3stop =[]\n",
    "    for j in top_3grams_nostop:\n",
    "        grams3stop.append(j[0])\n",
    "    grams3pun =[]\n",
    "    for j in top_3grams_nopun:\n",
    "        grams3pun.append(j[0])\n",
    "    \n",
    "    grams2stop =[]\n",
    "    for j in top_2grams_nostop:\n",
    "        grams2stop.append(j[0])\n",
    "    grams2pun =[]\n",
    "    for j in top_2grams_nopun:\n",
    "        grams2pun.append(j[0])\n",
    "        \n",
    "\n",
    "    \n",
    "    g3stop.append(grams3stop)\n",
    "    g3pun.append(grams3pun)\n",
    "    g2stop.append(grams2stop)\n",
    "    g2pun.append(grams2pun)\n",
    "    \n",
    "    \n",
    "    \n",
    "flat_g3stop = list(chain.from_iterable(g3stop))\n",
    "flat_g3spun = list(chain.from_iterable(g3pun))\n",
    "\n",
    "flat_g2stop = list(chain.from_iterable(g2stop))\n",
    "flat_g2spun = list(chain.from_iterable(g2pun))\n",
    "\n",
    "  \n",
    "STOP3_df = pd.Series(flat_g3stop).value_counts()\n",
    "PUN3_df = pd.Series(flat_g3spun).value_counts()\n",
    "\n",
    "STOP2_df = pd.Series(flat_g2stop).value_counts()\n",
    "PUN2_df = pd.Series(flat_g2spun).value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e43358f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(top, post, time)        3\n",
       "(gon, na, win)           3\n",
       "(indians, blew, lead)    2\n",
       "(13, year, old)          2\n",
       "(rip, kobe, bryant)      2\n",
       "(wan, na, see)           2\n",
       "(got, ta, love)          2\n",
       "(gon, na, end)           2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOP3_df[STOP3_df > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82d6c672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(one, of, the)       9\n",
       "(this, is, the)      9\n",
       "(this, is, a)        8\n",
       "(a, lot, of)         7\n",
       "(to, be, a)          7\n",
       "(going, to, be)      7\n",
       "(is, going, to)      6\n",
       "(what, the, fuck)    6\n",
       "(gon, na, be)        6\n",
       "(of, all, time)      6\n",
       "(i, don, t)          6\n",
       "(is, gon, na)        6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUN3_df[PUN3_df > (len(SportsSubs)-4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0f2e167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(gon, na)        9\n",
       "(holy, shit)     8\n",
       "(ever, seen)     6\n",
       "(got, ta)        6\n",
       "(feel, like)     6\n",
       "(years, ago)     6\n",
       "(looks, like)    6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOP2_df[STOP2_df > (len(SportsSubs)-4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86411463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(this, is)     9\n",
       "(one, of)      9\n",
       "(at, the)      9\n",
       "(the, best)    9\n",
       "(all, the)     9\n",
       "(it, was)      9\n",
       "(i, do)        9\n",
       "(it, s)        9\n",
       "(is, a)        9\n",
       "(going, to)    9\n",
       "(is, the)      9\n",
       "(and, the)     9\n",
       "(of, the)      9\n",
       "(for, the)     9\n",
       "(and, i)       9\n",
       "(to, the)      9\n",
       "(in, the)      9\n",
       "(i, was)       9\n",
       "(on, the)      9\n",
       "(to, be)       9\n",
       "(i, m)         9\n",
       "(for, a)       8\n",
       "(in, a)        8\n",
       "(i, think)     8\n",
       "(what, a)      8\n",
       "(as, a)        8\n",
       "(gon, na)      8\n",
       "(but, i)       8\n",
       "(to, see)      8\n",
       "(i, am)        7\n",
       "(the, most)    7\n",
       "(the, game)    7\n",
       "(he, was)      7\n",
       "(don, t)       7\n",
       "(from, the)    7\n",
       "(i, just)      7\n",
       "(will, be)     7\n",
       "(of, a)        7\n",
       "(was, a)       7\n",
       "(with, the)    7\n",
       "(be, a)        7\n",
       "(if, you)      7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUN2_df[PUN2_df > (len(SportsSubs)-3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d60f2d",
   "metadata": {},
   "source": [
    "### Movies Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "46acc3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   movies\n",
      "Processing:   harrypotter\n",
      "Processing:   StarWars\n",
      "Processing:   anime\n",
      "Processing:   batman\n",
      "Processing:   moviecritic\n",
      "Processing:   MovieSuggestions\n",
      "Processing:   Hungergames\n",
      "Processing:   MarvelStudiosSpoilers\n",
      "(best, movie, ever)        2\n",
      "(max, fury, road)          2\n",
      "(horror, movie, ever)      2\n",
      "(yes, yes, yes)            2\n",
      "(every, time, watch)       2\n",
      "(one, favorite, films)     2\n",
      "(may, rest, peace)         2\n",
      "(amazing, well, done)      2\n",
      "(ca, wait, see)            2\n",
      "(mad, max, fury)           2\n",
      "(gon, na, say)             2\n",
      "(one, favorite, movies)    2\n",
      "(one, best, movies)        2\n",
      "(best, movies, ever)       2\n",
      "(top, post, time)          2\n",
      "(sure, welcome, one)       2\n",
      "(watched, last, night)     2\n",
      "(movie, ever, seen)        2\n",
      "dtype: int64\n",
      "(i, don, t)        8\n",
      "(one, of, my)      8\n",
      "(one, of, the)     8\n",
      "(i, feel, like)    7\n",
      "(this, is, the)    7\n",
      "(a, lot, of)       6\n",
      "(to, be, a)        6\n",
      "(of, all, time)    6\n",
      "dtype: int64\n",
      "(gon, na)          9\n",
      "(feel, like)       8\n",
      "(looks, like)      6\n",
      "(first, time)      6\n",
      "(one, favorite)    6\n",
      "(years, ago)       6\n",
      "dtype: int64\n",
      "(i, m)         9\n",
      "(this, is)     9\n",
      "(and, i)       9\n",
      "(one, of)      9\n",
      "(for, the)     9\n",
      "(it, was)      9\n",
      "(i, was)       9\n",
      "(on, the)      9\n",
      "(in, the)      9\n",
      "(to, be)       9\n",
      "(of, the)      9\n",
      "(it, s)        9\n",
      "(is, a)        9\n",
      "(is, the)      8\n",
      "(but, i)       8\n",
      "(for, a)       8\n",
      "(i, have)      8\n",
      "(to, the)      8\n",
      "(as, a)        8\n",
      "(the, same)    8\n",
      "(in, a)        8\n",
      "(i, love)      8\n",
      "(when, i)      7\n",
      "(was, a)       7\n",
      "(i, think)     7\n",
      "(i, do)        7\n",
      "(i, just)      7\n",
      "(i, can)       7\n",
      "(at, the)      7\n",
      "(to, see)      7\n",
      "(out, of)      7\n",
      "(and, the)     7\n",
      "(with, the)    7\n",
      "dtype: int64\n",
      "CPU times: total: 37.5 s\n",
      "Wall time: 38.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "g3stop = []\n",
    "g3pun = []\n",
    "g2stop = []\n",
    "g2pun = []\n",
    "\n",
    "\n",
    "for i in MoviesSubs:\n",
    "    print(\"Processing:  \", i)\n",
    "    ulysses_txt = load_txt(f'{i}.txt')\n",
    "    ulysses_tokens = nltk.word_tokenize(ulysses_txt)\n",
    "\n",
    "    ulysses_lowercase = [t.lower() for t in ulysses_tokens]\n",
    "    ulysses_no_punct = [t for t in ulysses_lowercase if t.isalnum()]\n",
    "    ulysses_no_stop = [t for t in ulysses_no_punct if t not in stopwords.words('english')]\n",
    "    \n",
    "    c3stop = Counter(ngrams(ulysses_no_stop, n=3))\n",
    "    c3pun = Counter(ngrams(ulysses_no_punct, n=3))\n",
    "    \n",
    "    c2stop = Counter(ngrams(ulysses_no_stop, n=2))\n",
    "    c2pun = Counter(ngrams(ulysses_no_punct, n=2))\n",
    "    \n",
    "    \n",
    "    top_3grams_nostop = c3stop.most_common(100)    \n",
    "    top_3grams_nopun = c3pun.most_common(100)\n",
    "    \n",
    "    top_2grams_nostop = c2stop.most_common(100)    \n",
    "    top_2grams_nopun = c2pun.most_common(100)\n",
    "    \n",
    "    grams3stop =[]\n",
    "    for j in top_3grams_nostop:\n",
    "        grams3stop.append(j[0])\n",
    "    grams3pun =[]\n",
    "    for j in top_3grams_nopun:\n",
    "        grams3pun.append(j[0])\n",
    "    \n",
    "    grams2stop =[]\n",
    "    for j in top_2grams_nostop:\n",
    "        grams2stop.append(j[0])\n",
    "    grams2pun =[]\n",
    "    for j in top_2grams_nopun:\n",
    "        grams2pun.append(j[0])\n",
    "        \n",
    "\n",
    "    \n",
    "    g3stop.append(grams3stop)\n",
    "    g3pun.append(grams3pun)\n",
    "    g2stop.append(grams2stop)\n",
    "    g2pun.append(grams2pun)\n",
    "    \n",
    "    \n",
    "    \n",
    "flat_g3stop = list(chain.from_iterable(g3stop))\n",
    "flat_g3spun = list(chain.from_iterable(g3pun))\n",
    "\n",
    "flat_g2stop = list(chain.from_iterable(g2stop))\n",
    "flat_g2spun = list(chain.from_iterable(g2pun))\n",
    "\n",
    "  \n",
    "STOP3_df = pd.Series(flat_g3stop).value_counts()\n",
    "PUN3_df = pd.Series(flat_g3spun).value_counts()\n",
    "\n",
    "STOP2_df = pd.Series(flat_g2stop).value_counts()\n",
    "PUN2_df = pd.Series(flat_g2spun).value_counts()\n",
    "\n",
    "\n",
    "print(STOP3_df[STOP3_df > 1])\n",
    "print(PUN3_df[PUN3_df > (len(SportsSubs)-4)])\n",
    "\n",
    "print(STOP2_df[STOP2_df > (len(SportsSubs)-4)])\n",
    "print(PUN2_df[PUN2_df > (len(SportsSubs)-3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10352aa3",
   "metadata": {},
   "source": [
    "### New Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ec2aa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   politics\n",
      "Processing:   worldnews\n",
      "Processing:   news\n",
      "Processing:   conspiracy\n",
      "Processing:   Libertarian\n",
      "Processing:   offbeat\n",
      "Processing:   TrueReddit\n",
      "Processing:   Conservative\n",
      "(contact, moderators, subreddit)      4\n",
      "(action, performed, automatically)    4\n",
      "(subreddit, questions, concerns)      4\n",
      "(moderators, subreddit, questions)    4\n",
      "(please, contact, moderators)         4\n",
      "(automatically, please, contact)      4\n",
      "(performed, automatically, please)    4\n",
      "(bot, action, performed)              4\n",
      "(president, united, states)           3\n",
      "(would, love, see)                    2\n",
      "(queen, elizabeth, ii)                2\n",
      "(https, https, https)                 2\n",
      "(na, na, na)                          2\n",
      "dtype: int64\n",
      "(a, lot, of)         8\n",
      "(this, is, the)      8\n",
      "(one, of, the)       8\n",
      "(this, is, a)        8\n",
      "(is, going, to)      7\n",
      "(the, rest, of)      7\n",
      "(the, fact, that)    6\n",
      "(i, don, t)          6\n",
      "(to, be, a)          6\n",
      "dtype: int64\n",
      "(gon, na)           8\n",
      "(https, https)      6\n",
      "(united, states)    6\n",
      "(feel, like)        6\n",
      "dtype: int64\n",
      "(of, the)      8\n",
      "(is, the)      8\n",
      "(would, be)    8\n",
      "(of, a)        8\n",
      "(at, the)      8\n",
      "(with, the)    8\n",
      "(it, was)      8\n",
      "(that, the)    8\n",
      "(out, of)      8\n",
      "(for, a)       8\n",
      "(in, the)      8\n",
      "(they, are)    8\n",
      "(and, the)     8\n",
      "(in, a)        8\n",
      "(it, s)        8\n",
      "(all, the)     8\n",
      "(to, be)       8\n",
      "(to, the)      8\n",
      "(this, is)     8\n",
      "(on, the)      8\n",
      "(for, the)     8\n",
      "(is, a)        8\n",
      "(if, you)      8\n",
      "(going, to)    8\n",
      "(he, was)      7\n",
      "(have, a)      7\n",
      "(i, m)         7\n",
      "(to, see)      7\n",
      "(as, a)        7\n",
      "(one, of)      7\n",
      "(i, am)        7\n",
      "(but, i)       7\n",
      "(was, a)       7\n",
      "(from, the)    7\n",
      "(it, is)       7\n",
      "(to, do)       7\n",
      "(i, think)     7\n",
      "(need, to)     7\n",
      "(to, get)      7\n",
      "(and, i)       7\n",
      "(and, it)      7\n",
      "(by, the)      7\n",
      "dtype: int64\n",
      "CPU times: total: 34.5 s\n",
      "Wall time: 41.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "g3stop = []\n",
    "g3pun = []\n",
    "g2stop = []\n",
    "g2pun = []\n",
    "\n",
    "\n",
    "for i in NewsSubs:\n",
    "    print(\"Processing:  \", i)\n",
    "    ulysses_txt = load_txt(f'{i}.txt')\n",
    "    ulysses_tokens = nltk.word_tokenize(ulysses_txt)\n",
    "\n",
    "    ulysses_lowercase = [t.lower() for t in ulysses_tokens]\n",
    "    ulysses_no_punct = [t for t in ulysses_lowercase if t.isalnum()]\n",
    "    ulysses_no_stop = [t for t in ulysses_no_punct if t not in stopwords.words('english')]\n",
    "    \n",
    "    c3stop = Counter(ngrams(ulysses_no_stop, n=3))\n",
    "    c3pun = Counter(ngrams(ulysses_no_punct, n=3))\n",
    "    \n",
    "    c2stop = Counter(ngrams(ulysses_no_stop, n=2))\n",
    "    c2pun = Counter(ngrams(ulysses_no_punct, n=2))\n",
    "    \n",
    "    \n",
    "    top_3grams_nostop = c3stop.most_common(100)    \n",
    "    top_3grams_nopun = c3pun.most_common(100)\n",
    "    \n",
    "    top_2grams_nostop = c2stop.most_common(100)    \n",
    "    top_2grams_nopun = c2pun.most_common(100)\n",
    "    \n",
    "    grams3stop =[]\n",
    "    for j in top_3grams_nostop:\n",
    "        grams3stop.append(j[0])\n",
    "    grams3pun =[]\n",
    "    for j in top_3grams_nopun:\n",
    "        grams3pun.append(j[0])\n",
    "    \n",
    "    grams2stop =[]\n",
    "    for j in top_2grams_nostop:\n",
    "        grams2stop.append(j[0])\n",
    "    grams2pun =[]\n",
    "    for j in top_2grams_nopun:\n",
    "        grams2pun.append(j[0])\n",
    "        \n",
    "\n",
    "    \n",
    "    g3stop.append(grams3stop)\n",
    "    g3pun.append(grams3pun)\n",
    "    g2stop.append(grams2stop)\n",
    "    g2pun.append(grams2pun)\n",
    "    \n",
    "    \n",
    "    \n",
    "flat_g3stop = list(chain.from_iterable(g3stop))\n",
    "flat_g3spun = list(chain.from_iterable(g3pun))\n",
    "\n",
    "flat_g2stop = list(chain.from_iterable(g2stop))\n",
    "flat_g2spun = list(chain.from_iterable(g2pun))\n",
    "\n",
    "  \n",
    "STOP3_df = pd.Series(flat_g3stop).value_counts()\n",
    "PUN3_df = pd.Series(flat_g3spun).value_counts()\n",
    "\n",
    "STOP2_df = pd.Series(flat_g2stop).value_counts()\n",
    "PUN2_df = pd.Series(flat_g2spun).value_counts()\n",
    "\n",
    "\n",
    "print(STOP3_df[STOP3_df > 1])\n",
    "print(PUN3_df[PUN3_df > (len(SportsSubs)-4)])\n",
    "\n",
    "print(STOP2_df[STOP2_df > (len(SportsSubs)-4)])\n",
    "print(PUN2_df[PUN2_df > (len(SportsSubs)-3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c300c",
   "metadata": {},
   "source": [
    "### Learning Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26ddea12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   todayilearned\n",
      "Processing:   science\n",
      "Processing:   askscience\n",
      "Processing:   space\n",
      "Processing:   AskHistorians\n",
      "Processing:   YouShouldKnow\n",
      "Processing:   explainlikeimfive\n",
      "(https, https, https)                    4\n",
      "(removed, removed, removed)              4\n",
      "(free, speech, democracy)                2\n",
      "(5, years, ago)                          2\n",
      "(one, favorite, subs)                    2\n",
      "(keep, good, work)                       2\n",
      "(picture, black, hole)                   2\n",
      "(supermassive, black, hole)              2\n",
      "(may, rest, peace)                       2\n",
      "(innovation, free, speech)               2\n",
      "(net, neutrality, cornerstone)           2\n",
      "(neutrality, cornerstone, innovation)    2\n",
      "(cornerstone, innovation, free)          2\n",
      "dtype: int64\n",
      "(a, lot, of)         7\n",
      "(the, fact, that)    7\n",
      "(one, of, the)       7\n",
      "(this, is, a)        6\n",
      "(there, is, a)       6\n",
      "dtype: int64\n",
      "(years, ago)      6\n",
      "(https, https)    6\n",
      "(many, people)    6\n",
      "dtype: int64\n",
      "(in, the)      7\n",
      "(on, the)      7\n",
      "(this, is)     7\n",
      "(if, you)      7\n",
      "(i, do)        7\n",
      "(it, is)       7\n",
      "(one, of)      7\n",
      "(and, the)     7\n",
      "(is, the)      7\n",
      "(for, a)       7\n",
      "(is, a)        7\n",
      "(but, i)       7\n",
      "(for, the)     7\n",
      "(as, a)        7\n",
      "(with, the)    7\n",
      "(at, the)      7\n",
      "(and, i)       7\n",
      "(all, the)     7\n",
      "(to, be)       7\n",
      "(to, the)      7\n",
      "(it, was)      7\n",
      "(of, the)      7\n",
      "dtype: int64\n",
      "CPU times: total: 44.6 s\n",
      "Wall time: 48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "g3stop = []\n",
    "g3pun = []\n",
    "g2stop = []\n",
    "g2pun = []\n",
    "\n",
    "\n",
    "for i in LearningSubs:\n",
    "    print(\"Processing:  \", i)\n",
    "    ulysses_txt = load_txt(f'{i}.txt')\n",
    "    ulysses_tokens = nltk.word_tokenize(ulysses_txt)\n",
    "\n",
    "    ulysses_lowercase = [t.lower() for t in ulysses_tokens]\n",
    "    ulysses_no_punct = [t for t in ulysses_lowercase if t.isalnum()]\n",
    "    ulysses_no_stop = [t for t in ulysses_no_punct if t not in stopwords.words('english')]\n",
    "    \n",
    "    c3stop = Counter(ngrams(ulysses_no_stop, n=3))\n",
    "    c3pun = Counter(ngrams(ulysses_no_punct, n=3))\n",
    "    \n",
    "    c2stop = Counter(ngrams(ulysses_no_stop, n=2))\n",
    "    c2pun = Counter(ngrams(ulysses_no_punct, n=2))\n",
    "    \n",
    "    \n",
    "    top_3grams_nostop = c3stop.most_common(100)    \n",
    "    top_3grams_nopun = c3pun.most_common(100)\n",
    "    \n",
    "    top_2grams_nostop = c2stop.most_common(100)    \n",
    "    top_2grams_nopun = c2pun.most_common(100)\n",
    "    \n",
    "    grams3stop =[]\n",
    "    for j in top_3grams_nostop:\n",
    "        grams3stop.append(j[0])\n",
    "    grams3pun =[]\n",
    "    for j in top_3grams_nopun:\n",
    "        grams3pun.append(j[0])\n",
    "    \n",
    "    grams2stop =[]\n",
    "    for j in top_2grams_nostop:\n",
    "        grams2stop.append(j[0])\n",
    "    grams2pun =[]\n",
    "    for j in top_2grams_nopun:\n",
    "        grams2pun.append(j[0])\n",
    "        \n",
    "\n",
    "    \n",
    "    g3stop.append(grams3stop)\n",
    "    g3pun.append(grams3pun)\n",
    "    g2stop.append(grams2stop)\n",
    "    g2pun.append(grams2pun)\n",
    "    \n",
    "    \n",
    "    \n",
    "flat_g3stop = list(chain.from_iterable(g3stop))\n",
    "flat_g3spun = list(chain.from_iterable(g3pun))\n",
    "\n",
    "flat_g2stop = list(chain.from_iterable(g2stop))\n",
    "flat_g2spun = list(chain.from_iterable(g2pun))\n",
    "\n",
    "  \n",
    "STOP3_df = pd.Series(flat_g3stop).value_counts()\n",
    "PUN3_df = pd.Series(flat_g3spun).value_counts()\n",
    "\n",
    "STOP2_df = pd.Series(flat_g2stop).value_counts()\n",
    "PUN2_df = pd.Series(flat_g2spun).value_counts()\n",
    "\n",
    "\n",
    "print(STOP3_df[STOP3_df > 1])\n",
    "print(PUN3_df[PUN3_df > (len(SportsSubs)-4)])\n",
    "\n",
    "print(STOP2_df[STOP2_df > (len(SportsSubs)-4)])\n",
    "print(PUN2_df[PUN2_df > (len(SportsSubs)-3)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a4ac38",
   "metadata": {},
   "source": [
    "### Humour Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3e49caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   funny\n",
      "Processing:   fffffffuuuuuuuuuuuu\n",
      "Processing:   4chan\n",
      "Processing:   lmGoingToHellForThis\n",
      "Processing:   firstworldanarchists\n",
      "Processing:   circlejerk\n",
      "Processing:   okbuddyretard\n",
      "Processing:   facepalm\n",
      "Processing:   Jokes\n",
      "(gold, kind, stranger)      3\n",
      "(links, please, respect)    2\n",
      "(threads, info, contact)    2\n",
      "(bloop, someone, linked)    2\n",
      "(follow, links, please)     2\n",
      "(bleep, bloop, someone)     2\n",
      "(please, respect, rules)    2\n",
      "(respect, rules, reddit)    2\n",
      "(rules, reddit, vote)       2\n",
      "(reddit, vote, threads)     2\n",
      "(vote, threads, info)       2\n",
      "(bot, bleep, bloop)         2\n",
      "(thing, ever, seen)         2\n",
      "(thanks, gold, kind)        2\n",
      "(edit, thanks, gold)        2\n",
      "dtype: int64\n",
      "(this, is, the)    8\n",
      "(this, is, a)      6\n",
      "dtype: int64\n",
      "(looks, like)     8\n",
      "(gon, na)         8\n",
      "(sounds, like)    6\n",
      "(front, page)     6\n",
      "dtype: int64\n",
      "(this, is)     9\n",
      "(to, the)      9\n",
      "(in, the)      9\n",
      "(in, a)        9\n",
      "(if, you)      9\n",
      "(is, the)      9\n",
      "(to, get)      9\n",
      "(for, the)     9\n",
      "(to, be)       9\n",
      "(on, the)      9\n",
      "(of, the)      9\n",
      "(i, have)      8\n",
      "(all, the)     8\n",
      "(is, a)        8\n",
      "(and, the)     8\n",
      "(i, think)     8\n",
      "(out, of)      8\n",
      "(and, i)       8\n",
      "(is, this)     7\n",
      "(to, do)       7\n",
      "(i, do)        7\n",
      "(at, the)      7\n",
      "(i, was)       7\n",
      "(but, i)       7\n",
      "(was, a)       7\n",
      "(it, was)      7\n",
      "(going, to)    7\n",
      "dtype: int64\n",
      "CPU times: total: 17.2 s\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "g3stop = []\n",
    "g3pun = []\n",
    "g2stop = []\n",
    "g2pun = []\n",
    "\n",
    "\n",
    "for i in HumourSubs:\n",
    "    print(\"Processing:  \", i)\n",
    "    ulysses_txt = load_txt(f'{i}.txt')\n",
    "    ulysses_tokens = nltk.word_tokenize(ulysses_txt)\n",
    "\n",
    "    ulysses_lowercase = [t.lower() for t in ulysses_tokens]\n",
    "    ulysses_no_punct = [t for t in ulysses_lowercase if t.isalnum()]\n",
    "    ulysses_no_stop = [t for t in ulysses_no_punct if t not in stopwords.words('english')]\n",
    "    \n",
    "    c3stop = Counter(ngrams(ulysses_no_stop, n=3))\n",
    "    c3pun = Counter(ngrams(ulysses_no_punct, n=3))\n",
    "    \n",
    "    c2stop = Counter(ngrams(ulysses_no_stop, n=2))\n",
    "    c2pun = Counter(ngrams(ulysses_no_punct, n=2))\n",
    "    \n",
    "    \n",
    "    top_3grams_nostop = c3stop.most_common(100)    \n",
    "    top_3grams_nopun = c3pun.most_common(100)\n",
    "    \n",
    "    top_2grams_nostop = c2stop.most_common(100)    \n",
    "    top_2grams_nopun = c2pun.most_common(100)\n",
    "    \n",
    "    grams3stop =[]\n",
    "    for j in top_3grams_nostop:\n",
    "        grams3stop.append(j[0])\n",
    "    grams3pun =[]\n",
    "    for j in top_3grams_nopun:\n",
    "        grams3pun.append(j[0])\n",
    "    \n",
    "    grams2stop =[]\n",
    "    for j in top_2grams_nostop:\n",
    "        grams2stop.append(j[0])\n",
    "    grams2pun =[]\n",
    "    for j in top_2grams_nopun:\n",
    "        grams2pun.append(j[0])\n",
    "        \n",
    "\n",
    "    \n",
    "    g3stop.append(grams3stop)\n",
    "    g3pun.append(grams3pun)\n",
    "    g2stop.append(grams2stop)\n",
    "    g2pun.append(grams2pun)\n",
    "    \n",
    "    \n",
    "    \n",
    "flat_g3stop = list(chain.from_iterable(g3stop))\n",
    "flat_g3spun = list(chain.from_iterable(g3pun))\n",
    "\n",
    "flat_g2stop = list(chain.from_iterable(g2stop))\n",
    "flat_g2spun = list(chain.from_iterable(g2pun))\n",
    "\n",
    "  \n",
    "STOP3_df = pd.Series(flat_g3stop).value_counts()\n",
    "PUN3_df = pd.Series(flat_g3spun).value_counts()\n",
    "\n",
    "STOP2_df = pd.Series(flat_g2stop).value_counts()\n",
    "PUN2_df = pd.Series(flat_g2spun).value_counts()\n",
    "\n",
    "\n",
    "print(STOP3_df[STOP3_df > 1])\n",
    "print(PUN3_df[PUN3_df > (len(SportsSubs)-4)])\n",
    "\n",
    "print(STOP2_df[STOP2_df > (len(SportsSubs)-4)])\n",
    "print(PUN2_df[PUN2_df > (len(SportsSubs)-3)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f959fd",
   "metadata": {},
   "source": [
    "### TV Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f905fa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   arresteddevelopment\n",
      "Processing:   gameofthrones\n",
      "Processing:   doctorwho\n",
      "Processing:   mylittlepony\n",
      "Processing:   community\n",
      "Processing:   breakingbad\n",
      "Processing:   adventuretime\n",
      "(may, rest, peace)       3\n",
      "(https, https, https)    2\n",
      "(one, best, actors)      2\n",
      "(gon, na, get)           2\n",
      "dtype: int64\n",
      "(this, is, the)    7\n",
      "(one, of, the)     7\n",
      "(a, lot, of)       6\n",
      "(this, is, a)      6\n",
      "(i, feel, like)    6\n",
      "(one, of, my)      6\n",
      "dtype: int64\n",
      "(holy, shit)     7\n",
      "(looks, like)    7\n",
      "(gon, na)        7\n",
      "(feel, like)     6\n",
      "dtype: int64\n",
      "(i, was)       7\n",
      "(one, of)      7\n",
      "(i, have)      7\n",
      "(the, best)    7\n",
      "(with, the)    7\n",
      "(of, the)      7\n",
      "(to, the)      7\n",
      "(is, a)        7\n",
      "(i, love)      7\n",
      "(the, show)    7\n",
      "(going, to)    7\n",
      "(in, a)        7\n",
      "(it, was)      7\n",
      "(for, the)     7\n",
      "(and, the)     7\n",
      "(i, just)      7\n",
      "(this, is)     7\n",
      "(and, i)       7\n",
      "(in, the)      7\n",
      "(to, be)       7\n",
      "(on, the)      7\n",
      "dtype: int64\n",
      "CPU times: total: 20.1 s\n",
      "Wall time: 21.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "g3stop = []\n",
    "g3pun = []\n",
    "g2stop = []\n",
    "g2pun = []\n",
    "\n",
    "\n",
    "for i in TVSubs:\n",
    "    print(\"Processing:  \", i)\n",
    "    ulysses_txt = load_txt(f'{i}.txt')\n",
    "    ulysses_tokens = nltk.word_tokenize(ulysses_txt)\n",
    "\n",
    "    ulysses_lowercase = [t.lower() for t in ulysses_tokens]\n",
    "    ulysses_no_punct = [t for t in ulysses_lowercase if t.isalnum()]\n",
    "    ulysses_no_stop = [t for t in ulysses_no_punct if t not in stopwords.words('english')]\n",
    "    \n",
    "    c3stop = Counter(ngrams(ulysses_no_stop, n=3))\n",
    "    c3pun = Counter(ngrams(ulysses_no_punct, n=3))\n",
    "    \n",
    "    c2stop = Counter(ngrams(ulysses_no_stop, n=2))\n",
    "    c2pun = Counter(ngrams(ulysses_no_punct, n=2))\n",
    "    \n",
    "    \n",
    "    top_3grams_nostop = c3stop.most_common(100)    \n",
    "    top_3grams_nopun = c3pun.most_common(100)\n",
    "    \n",
    "    top_2grams_nostop = c2stop.most_common(100)    \n",
    "    top_2grams_nopun = c2pun.most_common(100)\n",
    "    \n",
    "    grams3stop =[]\n",
    "    for j in top_3grams_nostop:\n",
    "        grams3stop.append(j[0])\n",
    "    grams3pun =[]\n",
    "    for j in top_3grams_nopun:\n",
    "        grams3pun.append(j[0])\n",
    "    \n",
    "    grams2stop =[]\n",
    "    for j in top_2grams_nostop:\n",
    "        grams2stop.append(j[0])\n",
    "    grams2pun =[]\n",
    "    for j in top_2grams_nopun:\n",
    "        grams2pun.append(j[0])\n",
    "        \n",
    "\n",
    "    \n",
    "    g3stop.append(grams3stop)\n",
    "    g3pun.append(grams3pun)\n",
    "    g2stop.append(grams2stop)\n",
    "    g2pun.append(grams2pun)\n",
    "    \n",
    "    \n",
    "    \n",
    "flat_g3stop = list(chain.from_iterable(g3stop))\n",
    "flat_g3spun = list(chain.from_iterable(g3pun))\n",
    "\n",
    "flat_g2stop = list(chain.from_iterable(g2stop))\n",
    "flat_g2spun = list(chain.from_iterable(g2pun))\n",
    "\n",
    "  \n",
    "STOP3_df = pd.Series(flat_g3stop).value_counts()\n",
    "PUN3_df = pd.Series(flat_g3spun).value_counts()\n",
    "\n",
    "STOP2_df = pd.Series(flat_g2stop).value_counts()\n",
    "PUN2_df = pd.Series(flat_g2spun).value_counts()\n",
    "\n",
    "\n",
    "print(STOP3_df[STOP3_df > 1])\n",
    "print(PUN3_df[PUN3_df > (len(SportsSubs)-4)])\n",
    "\n",
    "print(STOP2_df[STOP2_df > (len(SportsSubs)-4)])\n",
    "print(PUN2_df[PUN2_df > (len(SportsSubs)-3)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0a627e",
   "metadata": {},
   "source": [
    "### Gaming Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "075a22b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   gaming\n",
      "Processing:   leagueoflegends\n",
      "Processing:   pokemon\n",
      "Processing:   Minecraft\n",
      "Processing:   starcraft\n",
      "Processing:   Games\n",
      "Processing:   DotA2\n",
      "Processing:   skyrim\n",
      "Processing:   tf2\n",
      "Processing:   magicTCG\n",
      "Processing:   wow\n",
      "Processing:   KerbalSpaceProgram\n",
      "Processing:   mindcrack\n",
      "(rest, peace, john)           5\n",
      "(holy, fucking, shit)         3\n",
      "(thing, ever, seen)           3\n",
      "(rest, peace, tb)             3\n",
      "(may, rest, peace)            3\n",
      "(https, https, https)         3\n",
      "(peace, rest, peace)          3\n",
      "(starcraft, 2, esports)       2\n",
      "(post, reddit, recorded)      2\n",
      "(top, post, reddit)           2\n",
      "(hey, top, post)              2\n",
      "(gon, na, need)               2\n",
      "(best, thing, ever)           2\n",
      "(young, rest, peace)          2\n",
      "(awesome, great, job)         2\n",
      "(ca, wait, see)               2\n",
      "(would, love, see)            2\n",
      "(recorded, top, posts)        2\n",
      "(reaching, day, top)          2\n",
      "(congrats, reaching, day)     2\n",
      "(got, ta, say)                2\n",
      "(2, years, ago)               2\n",
      "(reddit, recorded, top)       2\n",
      "(redditors, find, related)    2\n",
      "(really, well, done)          2\n",
      "(thread, watch, playlist)     2\n",
      "(would, like, know)           2\n",
      "(keep, good, work)            2\n",
      "(man, rest, peace)            2\n",
      "(loss, sorry, loss)           2\n",
      "(sorry, loss, sorry)          2\n",
      "(videos, thread, watch)       2\n",
      "(watch, playlist, 9654)       2\n",
      "(related, videos, watch)      2\n",
      "(playlist, 9654, http)        2\n",
      "(bot, working, hard)          2\n",
      "(working, hard, help)         2\n",
      "(hard, help, redditors)       2\n",
      "(help, redditors, find)       2\n",
      "(find, related, videos)       2\n",
      "(long, time, ago)             2\n",
      "(like, know, location)        2\n",
      "(star, wars, battlefront)     2\n",
      "(rest, peace, rest)           2\n",
      "dtype: int64\n",
      "(this, is, the)      13\n",
      "(one, of, the)       12\n",
      "(a, lot, of)         12\n",
      "(this, is, a)        12\n",
      "(going, to, be)       9\n",
      "(to, be, a)           9\n",
      "(i, do, know)         8\n",
      "(i, want, to)         8\n",
      "(thank, you, for)     7\n",
      "(one, of, my)         7\n",
      "(rest, in, peace)     7\n",
      "(of, the, best)       7\n",
      "(i, feel, like)       6\n",
      "(some, of, the)       6\n",
      "(is, going, to)       6\n",
      "(i, don, t)           6\n",
      "(this, is, so)        6\n",
      "(when, i, was)        6\n",
      "dtype: int64\n",
      "(gon, na)            12\n",
      "(got, ta)            11\n",
      "(looks, like)        11\n",
      "(feel, like)         11\n",
      "(holy, shit)         10\n",
      "(long, time)         10\n",
      "(oh, god)             8\n",
      "(something, like)     8\n",
      "(first, time)         8\n",
      "(years, ago)          8\n",
      "(ever, seen)          8\n",
      "(rest, peace)         7\n",
      "(https, https)        7\n",
      "(even, though)        7\n",
      "(well, done)          6\n",
      "(playing, game)       6\n",
      "(look, like)          6\n",
      "(many, people)        6\n",
      "dtype: int64\n",
      "(this, is)    13\n",
      "(on, the)     13\n",
      "(i, just)     13\n",
      "(and, the)    13\n",
      "(i, have)     13\n",
      "              ..\n",
      "(rest, in)     7\n",
      "(that, i)      7\n",
      "(it, is)       7\n",
      "(i, hope)      7\n",
      "(of, this)     7\n",
      "Length: 68, dtype: int64\n",
      "CPU times: total: 1min 1s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "g3stop = []\n",
    "g3pun = []\n",
    "g2stop = []\n",
    "g2pun = []\n",
    "\n",
    "\n",
    "for i in GamingSubs:\n",
    "    print(\"Processing:  \", i)\n",
    "    ulysses_txt = load_txt(f'{i}.txt')\n",
    "    ulysses_tokens = nltk.word_tokenize(ulysses_txt)\n",
    "\n",
    "    ulysses_lowercase = [t.lower() for t in ulysses_tokens]\n",
    "    ulysses_no_punct = [t for t in ulysses_lowercase if t.isalnum()]\n",
    "    ulysses_no_stop = [t for t in ulysses_no_punct if t not in stopwords.words('english')]\n",
    "    \n",
    "    c3stop = Counter(ngrams(ulysses_no_stop, n=3))\n",
    "    c3pun = Counter(ngrams(ulysses_no_punct, n=3))\n",
    "    \n",
    "    c2stop = Counter(ngrams(ulysses_no_stop, n=2))\n",
    "    c2pun = Counter(ngrams(ulysses_no_punct, n=2))\n",
    "    \n",
    "    \n",
    "    top_3grams_nostop = c3stop.most_common(100)    \n",
    "    top_3grams_nopun = c3pun.most_common(100)\n",
    "    \n",
    "    top_2grams_nostop = c2stop.most_common(100)    \n",
    "    top_2grams_nopun = c2pun.most_common(100)\n",
    "    \n",
    "    grams3stop =[]\n",
    "    for j in top_3grams_nostop:\n",
    "        grams3stop.append(j[0])\n",
    "    grams3pun =[]\n",
    "    for j in top_3grams_nopun:\n",
    "        grams3pun.append(j[0])\n",
    "    \n",
    "    grams2stop =[]\n",
    "    for j in top_2grams_nostop:\n",
    "        grams2stop.append(j[0])\n",
    "    grams2pun =[]\n",
    "    for j in top_2grams_nopun:\n",
    "        grams2pun.append(j[0])\n",
    "        \n",
    "\n",
    "    \n",
    "    g3stop.append(grams3stop)\n",
    "    g3pun.append(grams3pun)\n",
    "    g2stop.append(grams2stop)\n",
    "    g2pun.append(grams2pun)\n",
    "    \n",
    "    \n",
    "    \n",
    "flat_g3stop = list(chain.from_iterable(g3stop))\n",
    "flat_g3spun = list(chain.from_iterable(g3pun))\n",
    "\n",
    "flat_g2stop = list(chain.from_iterable(g2stop))\n",
    "flat_g2spun = list(chain.from_iterable(g2pun))\n",
    "\n",
    "  \n",
    "STOP3_df = pd.Series(flat_g3stop).value_counts()\n",
    "PUN3_df = pd.Series(flat_g3spun).value_counts()\n",
    "\n",
    "STOP2_df = pd.Series(flat_g2stop).value_counts()\n",
    "PUN2_df = pd.Series(flat_g2spun).value_counts()\n",
    "\n",
    "\n",
    "print(STOP3_df[STOP3_df > 1])\n",
    "print(PUN3_df[PUN3_df > (len(SportsSubs)-4)])\n",
    "\n",
    "print(STOP2_df[STOP2_df > (len(SportsSubs)-4)])\n",
    "print(PUN2_df[PUN2_df > (len(SportsSubs)-3)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca76f374",
   "metadata": {},
   "source": [
    "### Discussion Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7740e587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:   CrazyIdeas\n",
      "Processing:   AskReddit\n",
      "Processing:   fatpeoplestories\n",
      "Processing:   DoesAnybodyElse\n",
      "Processing:   IAmA\n",
      "Processing:   bestof\n",
      "Processing:   TalesFromRetail\n",
      "(https, https, https)       3\n",
      "(every, single, time)       2\n",
      "(net, neutrality, rules)    2\n",
      "(save, net, neutrality)     2\n",
      "(net, neutrality, https)    2\n",
      "(yes, yes, yes)             2\n",
      "dtype: int64\n",
      "(a, lot, of)       7\n",
      "(this, is, a)      6\n",
      "(one, of, the)     6\n",
      "(this, is, the)    6\n",
      "dtype: int64\n",
      "(feel, like)    7\n",
      "(gon, na)       6\n",
      "dtype: int64\n",
      "(but, i)      7\n",
      "(for, the)    7\n",
      "(if, you)     7\n",
      "(in, a)       7\n",
      "(and, i)      7\n",
      "(and, the)    7\n",
      "(all, the)    7\n",
      "(have, a)     7\n",
      "(of, the)     7\n",
      "(on, the)     7\n",
      "(to, be)      7\n",
      "(i, have)     7\n",
      "(to, the)     7\n",
      "(to, get)     7\n",
      "(to, do)      7\n",
      "(in, the)     7\n",
      "(this, is)    7\n",
      "(i, do)       7\n",
      "dtype: int64\n",
      "CPU times: total: 30.2 s\n",
      "Wall time: 30.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "g3stop = []\n",
    "g3pun = []\n",
    "g2stop = []\n",
    "g2pun = []\n",
    "\n",
    "\n",
    "for i in DiscussionSubs:\n",
    "    print(\"Processing:  \", i)\n",
    "    ulysses_txt = load_txt(f'{i}.txt')\n",
    "    ulysses_tokens = nltk.word_tokenize(ulysses_txt)\n",
    "\n",
    "    ulysses_lowercase = [t.lower() for t in ulysses_tokens]\n",
    "    ulysses_no_punct = [t for t in ulysses_lowercase if t.isalnum()]\n",
    "    ulysses_no_stop = [t for t in ulysses_no_punct if t not in stopwords.words('english')]\n",
    "    \n",
    "    c3stop = Counter(ngrams(ulysses_no_stop, n=3))\n",
    "    c3pun = Counter(ngrams(ulysses_no_punct, n=3))\n",
    "    \n",
    "    c2stop = Counter(ngrams(ulysses_no_stop, n=2))\n",
    "    c2pun = Counter(ngrams(ulysses_no_punct, n=2))\n",
    "    \n",
    "    \n",
    "    top_3grams_nostop = c3stop.most_common(100)    \n",
    "    top_3grams_nopun = c3pun.most_common(100)\n",
    "    \n",
    "    top_2grams_nostop = c2stop.most_common(100)    \n",
    "    top_2grams_nopun = c2pun.most_common(100)\n",
    "    \n",
    "    grams3stop =[]\n",
    "    for j in top_3grams_nostop:\n",
    "        grams3stop.append(j[0])\n",
    "    grams3pun =[]\n",
    "    for j in top_3grams_nopun:\n",
    "        grams3pun.append(j[0])\n",
    "    \n",
    "    grams2stop =[]\n",
    "    for j in top_2grams_nostop:\n",
    "        grams2stop.append(j[0])\n",
    "    grams2pun =[]\n",
    "    for j in top_2grams_nopun:\n",
    "        grams2pun.append(j[0])\n",
    "        \n",
    "\n",
    "    \n",
    "    g3stop.append(grams3stop)\n",
    "    g3pun.append(grams3pun)\n",
    "    g2stop.append(grams2stop)\n",
    "    g2pun.append(grams2pun)\n",
    "    \n",
    "    \n",
    "    \n",
    "flat_g3stop = list(chain.from_iterable(g3stop))\n",
    "flat_g3spun = list(chain.from_iterable(g3pun))\n",
    "\n",
    "flat_g2stop = list(chain.from_iterable(g2stop))\n",
    "flat_g2spun = list(chain.from_iterable(g2pun))\n",
    "\n",
    "  \n",
    "STOP3_df = pd.Series(flat_g3stop).value_counts()\n",
    "PUN3_df = pd.Series(flat_g3spun).value_counts()\n",
    "\n",
    "STOP2_df = pd.Series(flat_g2stop).value_counts()\n",
    "PUN2_df = pd.Series(flat_g2spun).value_counts()\n",
    "\n",
    "\n",
    "print(STOP3_df[STOP3_df > 1])\n",
    "print(PUN3_df[PUN3_df > (len(SportsSubs)-4)])\n",
    "\n",
    "print(STOP2_df[STOP2_df > (len(SportsSubs)-4)])\n",
    "print(PUN2_df[PUN2_df > (len(SportsSubs)-3)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70497519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a32f4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c54e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9988a8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bdd6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52551923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd5ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de5854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34134b66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967fd772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e8886e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758bcc31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b843dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e75740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc4bbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c97938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3520129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d975f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41429748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e22333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480abb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76060fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714923d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a909d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50fd0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d263d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f967647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffe02a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcbeb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d449aa58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d6f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca752a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c6d976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a030f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101554b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c2c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d6df99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b38d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_all_subs=[\"know\", \"get\", \"really\", \"got\", \"way\", \"even\", \"would\", \"love\", \"time\", \"like\", \"one\"]\n",
    "\n",
    "\n",
    "in_all_but_one=[\"people\", \"also\", \"good\", \"still\", \"think\", \"first\", \"see\", \"much\", \"well\", \"made\", \"great\"]\n",
    "\n",
    "in_all_but_two=[\"fucking\", \"ever\", \"man\", \"never\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
